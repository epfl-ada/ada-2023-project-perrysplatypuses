{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T22:04:05.183730Z",
     "iopub.status.busy": "2023-11-16T22:04:05.183555Z",
     "iopub.status.idle": "2023-11-16T22:04:05.197566Z",
     "shell.execute_reply": "2023-11-16T22:04:05.197108Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T22:04:05.199836Z",
     "iopub.status.busy": "2023-11-16T22:04:05.199502Z",
     "iopub.status.idle": "2023-11-16T22:04:05.594478Z",
     "shell.execute_reply": "2023-11-16T22:04:05.593913Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clustering import get_lda_clusters, get_vocab, word_topics_clustering, sort_meaningful, get_trf_clusters, topic_count\n",
    "from utils.clustering_evaluation import get_characters_with_tv_trop_info, variation_of_information, group_labels_by_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering methods comparison\n",
    "\n",
    "We use Variation of Information between our clusters and golden clusters from TV Tropes as suggested in [Learning Latent Personas of Film Characters](http://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf). This way we can compare our methods' performance with the original method performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA based clustering\n",
    "For the character names and linguistic features extraction pipeline, please refer to `extract_character_attributes.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_attributes =  pd.read_csv(\n",
    "    'data/character_attributes.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"adj\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), # need this to read list columns from csv\n",
    "        \"active\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "        \"patient\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "        }\n",
    "    )\n",
    "# select only the characters who have at least 3 liguistic features\n",
    "characters_attributes = sort_meaningful(characters_attributes, 3)\n",
    "\n",
    "characters_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_to_check, tv_tropes = get_characters_with_tv_trop_info(characters_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative_clusters_n = [25, 50, 100]\n",
    "n_components = [25, 50, 100]\n",
    "\n",
    "configs = {}\n",
    "config_base = {'characters': characters_to_check, 'min_freq': 5, 'max_freq':0.9}\n",
    "\n",
    "for alg_n in agglomerative_clusters_n:\n",
    "    for n in n_components:\n",
    "        config = config_base.copy()\n",
    "        config['clustering_algo'] = AgglomerativeClustering(n_clusters=alg_n, metric='cosine', linkage='complete')\n",
    "        config['n_components'] = n\n",
    "        configs[f'{alg_n} topics, {n} archetypes'] = config\n",
    "\n",
    "results_lda = {}\n",
    "for k, config in configs.items():\n",
    "    clusters = get_lda_clusters(**config)\n",
    "    results_lda[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes)\n",
    "    print(k, f'VI = {results_lda[k]}')\n",
    "\n",
    "clear_output(wait=True)\n",
    "results_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the results are even better (K=100, P=100, 5.42 in the paper and 4.9 here) than the results from the [paper](http://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf). That could indicate that using word2vec embeddings and Agglomerative clustering of the words to topics might be better suited for dividing the words into topics for the purpose of personas extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT based clustering\n",
    "\n",
    "For the embedding extraction see `utils/archive/transformer_embeddings.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_with_trf_emb =  pd.read_csv(\n",
    "    'data/trf_embeddings_for_labeled_characters.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"emb\": lambda x: [float(k) for k in x.strip(\"[]\").replace(\"'\",\"\").split(\", \")]\n",
    "        }\n",
    "    )\n",
    "# Leave only those, who we compared on the previous step\n",
    "characters_with_trf_emb = characters_with_trf_emb[characters_with_trf_emb['wiki_id'].isin(characters_to_check['wiki_id'].values)]\n",
    "\n",
    "characters_with_trf_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_to_check_trf, tv_tropes = get_characters_with_tv_trop_info(characters_with_trf_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_trf = {}\n",
    "for n in n_components:\n",
    "    k = f'{n} archetypes, agglomerative clustering'\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n, metric='euclidean', linkage='complete')\n",
    "    clusters = get_trf_clusters(characters_to_check_trf, agglomerative)\n",
    "    results_trf[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes)\n",
    "    print(k, f'VI = {results_trf[k]}')\n",
    "\n",
    "    k = f'{n} archetypes, kmeans clustering'\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    clusters = get_trf_clusters(characters_to_check_trf, kmeans)\n",
    "    results_trf[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes)\n",
    "    print(k, f'VI = {results_trf[k]}')\n",
    "\n",
    "clear_output(wait=True)\n",
    "results_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of BERT embeddings based clustering are also better than the results from the paper. Still, obtaining these embeddings is slow, and the difference between this and previous method is not that big, so we will stick to the faster and more interpretable LDA based method.\n",
    "\n",
    "**Comparison Table**\n",
    "|                        |Paper K=100, P=100| LDA-based clustering  K=100, P=100| BERT-based kmeans clustering P=100|\n",
    "|------------------------|------------------|-----------------------------------|-----------------------------------|\n",
    "|Variation of Information|              5.42|                               4.90|                              ~4.80|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "For the clusterization pipeline as well as the different clustering methods comparison refer to `clustering.ipynb`. For our initial analysis we will use 50 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T22:04:05.597362Z",
     "iopub.status.busy": "2023-11-16T22:04:05.596898Z",
     "iopub.status.idle": "2023-11-16T22:04:05.941635Z",
     "shell.execute_reply": "2023-11-16T22:04:05.940975Z"
    }
   },
   "outputs": [],
   "source": [
    "characters =  pd.read_csv(\n",
    "    'data/character_clusters.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"adj\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), # need this to read list columns from csv\n",
    "        \"active\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "        \"patient\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "        }\n",
    "    )\n",
    "characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T22:04:05.944085Z",
     "iopub.status.busy": "2023-11-16T22:04:05.943783Z",
     "iopub.status.idle": "2023-11-16T22:04:06.138476Z",
     "shell.execute_reply": "2023-11-16T22:04:06.137841Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"In the clustered characters dataframe there are {len(characters)} characters from {len(set(characters['wiki_id'].values))} movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\n",
    "    'data/MovieSummaries/movie.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'title', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_and_movies = characters.merge(movies, how='left', on='wiki_id')\n",
    "characters_and_movies = characters_and_movies[characters_and_movies['revenue'].notna()]\n",
    "\n",
    "print(f\"In the clustered characters with movie metadata dataframe there are {len(characters_and_movies)} characters from {len(set(characters_and_movies['wiki_id'].values))} movies with the revenue data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_and_movies.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T22:04:06.267874Z",
     "iopub.status.busy": "2023-11-16T22:04:06.267502Z",
     "iopub.status.idle": "2023-11-16T22:04:06.292394Z",
     "shell.execute_reply": "2023-11-16T22:04:06.291761Z"
    }
   },
   "outputs": [],
   "source": [
    "characters_and_movies[characters_and_movies['title'].str.contains(\"Batman\")][['title', 'character', 'cluster']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice, that, probably, cluster number 42 is the cluster of super-heroes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters interpretability\n",
    "For now we won't give particular names to each cluster. But we show, how to use data from Latent Dirichlet Allocation model to understand what is the meaning of each cluster. We can look at the most important topics (groups of words) for each cluster and conclude, what is the role of the character form a particular cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_dict = json.load(open('data/words_by_topic.json', 'r'))\n",
    "lda_components = np.load('data/lda_components.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 50, 10):\n",
    "    idx = lda_components[i].argmax()\n",
    "    feature_type = 'attribute'\n",
    "    if idx // 200 == 1:\n",
    "        feature_type = 'active verb'\n",
    "    elif idx // 200 == 2:\n",
    "        feature_type = 'patient verb'\n",
    "    print(f'For the cluster {i}, the most important topic is {feature_type} from')\n",
    "    print(topics_dict[str(idx % 200)])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can see, that characters in cluster 0 are the onces who move a lot, while in cluster 10 characters are usually someones relative and characters in cluster 40 are some authority figures. Further, we will look at the top topics for each cluster to interpret, what are the common traits of the characters in one cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial analysis of the actors' success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors = pd.read_csv(\n",
    "    'data/MovieSummaries/character.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'release_date', 'character', 'date_of_birth', 'sex', 'height', '.','actor','age','character_map','..','...','....']\n",
    ")\n",
    "actors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict_to_list = lambda x: [value for key, value in eval(x).items()]\n",
    "movies['languages'] = movies['languages'].apply(map_dict_to_list)\n",
    "movies['countries'] = movies['countries'].apply(map_dict_to_list)\n",
    "movies['genres'] = movies['genres'].apply(map_dict_to_list)\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which actors bring in the most money\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datas\n",
    "df_merged = actors.merge(movies, on=['wiki_id'], how='inner')\n",
    "\n",
    "# Clean data of movies without box offic revenue\n",
    "df_merged = df_merged.dropna(subset=['revenue'])\n",
    "\n",
    "# Determine the actor and sum the box office revenue\n",
    "actor_totalRevenue = df_merged.groupby(['actor'])['revenue'].agg(['sum', 'count']).reset_index()\n",
    "actor_totalRevenue.columns = ['actor', 'bo_revenue', 'Actor Count']\n",
    "\n",
    "# Sort the actor_revenue DataFrame in descending order\n",
    "actor_revenue_sort = actor_totalRevenue.sort_values(by='bo_revenue', ascending=False)\n",
    "\n",
    "#Extract only the top 20 best actors\n",
    "actor_top20 = actor_revenue_sort.head(20)\n",
    "\n",
    "# Merge the top 20 actors with original datas\n",
    "top20_actor_data = actor_top20.merge(df_merged[['actor', 'languages','sex']], on='actor', how='left')\n",
    "top20_actor_data = top20_actor_data.drop_duplicates(subset = ['actor'])\n",
    "top20_actor_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data above\n",
    "actor_names = top20_actor_data['actor']\n",
    "revenues_total = top20_actor_data['bo_revenue']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actor_names, revenues_total)\n",
    "plt.xlabel('Actor Names')\n",
    "plt.ylabel('Total Box Office Revenue')\n",
    "plt.title('Top 10 Actors by Box Office Revenue for the Movies They Acted In')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate the actor names for better readability\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data above, we are ranking the actors according to the sum of the box office revenue from the movies in which they acted, without considering whether it was a first, second, or episodic role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the main lead actors obtaining the highest revenue\n",
    "\n",
    "Let's consider the weight for every role as a share of words in the plot that is related to the specific character. Then, let's make the simple assumption that all revenue is created thanks to the characters from the plot and calculate how much money corresponds to every actor's role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculater_importance(x):\n",
    "    a = 0\n",
    "    for w in x[\"adj\"]:\n",
    "        a += w.isalpha()\n",
    "    for w in x[\"active\"]:\n",
    "        a += w.isalpha()\n",
    "    for w in x[\"patient\"]:\n",
    "        a += w.isalpha()\n",
    "    return a\n",
    "\n",
    "characters['importance'] = characters.apply(calculater_importance, axis=1)\n",
    "\n",
    "characters_with_importance = characters[['wiki_id', 'character', 'importance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_with_importance['importance_share'] = characters_with_importance['importance'] / characters_with_importance.groupby('wiki_id')['importance'].transform('sum')\n",
    "characters_with_importance['is_important'] = characters_with_importance['importance_share'] >= 0.2\n",
    "characters_with_importance.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_charac_actor = df_merged[['wiki_id', 'character','actor','revenue']]\n",
    "\n",
    "# Include characters and actors together in the same dataset\n",
    "character_actor = df_charac_actor.merge(characters_with_importance, on=['wiki_id', 'character'], how='left')\n",
    "\n",
    "# Merge only the main charac of movies\\n\",\n",
    "main_character = character_actor[character_actor['is_important'] == True]\n",
    "\n",
    "#Add up the Box office revenue and count the occurance of movie appearance\n",
    "main_charac_mergedBOR = main_character.groupby(['actor'])['revenue'].agg(['sum', 'count']).reset_index()\n",
    "\n",
    "main_charac_sort = main_charac_mergedBOR.sort_values(by='sum', ascending=False)\n",
    "\n",
    "main_charac_top20 = main_charac_sort.head(20)\n",
    "main_charac_top20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the charts for main actors\\n\",\n",
    "main_charac_top20_names = main_charac_top20['actor']\n",
    "main_charac_revenue = main_charac_top20['sum']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(main_charac_top20_names,main_charac_revenue)\n",
    "plt.xlabel('Actor Names')\n",
    "plt.ylabel('Box Office Revenue')\n",
    "plt.title('Top 20 Main Characters by Box average Office Revenue')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate the actor names for better readability\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top of the actors changed, and we see, that the actors look like those who we would expect to be successful (generally subjectively more famous than the previous top)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the language used in top 50 films / Which language drive the highest revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top 100 movies\n",
    "film_sort = df_merged[['countries','languages','revenue','wiki_id']]\n",
    "film_sort = film_sort.drop_duplicates(subset = ['wiki_id'])\n",
    "film_sort = film_sort.sort_values(by='revenue', ascending=False)\n",
    "top100film = film_sort.head(100)\n",
    "\n",
    "# Split the languages into new data frame\n",
    "df_split_languages = top100film.explode('languages')\n",
    "\n",
    "# Count the occurrence of the languages\n",
    "language_counts = df_split_languages['languages'].value_counts()\n",
    "df_language_counts = pd.DataFrame({'Language': language_counts.index, 'Count': language_counts.values})\n",
    "\n",
    "# Plot the data\n",
    "ax = df_language_counts.plot(kind='bar', x='Language', y='Count', figsize=(12, 6),logy=True)\n",
    "plt.title('How many movies in the given language are in top 100 movies by revenue')\n",
    "plt.xlabel('Language')\n",
    "plt.ylabel('Count (logscale)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.bar_label(ax.containers[0], label_type='edge')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a reasonable patter: English is almost everywhere, followed by wide-distributed languages like Spanish and French, and many languages that are used in only one movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study the genre of movies that earn the highest revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data to remove repeated movies\n",
    "df_clean_split_genres = df_merged[['title','genres','revenue','wiki_id']]\n",
    "df_clean_split_genres = df_clean_split_genres.drop_duplicates(subset = ['wiki_id'])\n",
    "\n",
    "# Split the genre into new data frames\n",
    "df_split_genre = df_clean_split_genres.explode('genres')[['title', 'revenue', 'genres']]\n",
    "\n",
    "# Find top genres\n",
    "top_genres = df_split_genre.groupby('genres').size().reset_index().sort_values(by=0, ascending=False)\n",
    "top_genres = top_genres.head(10)['genres'].values\n",
    "\n",
    "# Determine the actor and sum the box office revenue\n",
    "genre_totalRevenue = df_split_genre[df_split_genre['genres'].isin(top_genres)].groupby('genres')['revenue'].median().reset_index()\n",
    "\n",
    "# Sort the BOR in ascending order\n",
    "genre_totalRevenue = genre_totalRevenue.sort_values(by='revenue', ascending=False)\n",
    "\n",
    "genre = genre_totalRevenue['genres']\n",
    "genreRevenue = genre_totalRevenue['revenue']\n",
    "\n",
    "#Plot the charts\n",
    "plt.plot(genre, genreRevenue)\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Box office revenue')\n",
    "plt.title('Top 10 popular Genres sorted by median Box Office Revenue')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate the actor names for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre-cluster correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_and_movies_with_genres = characters.merge(movies, how='left', on='wiki_id')\n",
    "characters_and_movies_with_genres = characters_and_movies_with_genres[characters_and_movies_with_genres['genres'].notna()]\n",
    "characters_and_movies_with_genres = characters_and_movies_with_genres[['wiki_id', 'title', 'character', 'cluster', 'genres']]\n",
    "characters_and_movies_with_genres.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_and_movies_with_genres = characters_and_movies_with_genres.explode('genres')\n",
    "characters_and_movies_with_genres = characters_and_movies_with_genres[characters_and_movies_with_genres['genres'].isin(top_genres)]\n",
    "characters_and_movies_with_genres.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_genre_table = characters_and_movies_with_genres[['cluster', 'genres']].groupby(['cluster', 'genres']).size().reset_index()\n",
    "cluster_genre_table = cluster_genre_table.pivot(index='cluster', columns='genres', values=0)\n",
    "cluster_genre_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "table = cluster_genre_table[['Romantic comedy', 'Thriller']].values\n",
    "res = chi2_contingency(table)\n",
    "print(f'pvalue of the test with H0: cluster distribution is the same in Romantic comedy\tand Thriller films: {res[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revenue prediction using clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
