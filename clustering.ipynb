{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clustering import get_lda_clusters, get_kmeans_clusters, sort_meaningful, get_trf_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters =  pd.read_csv(\n",
    "    'data/character_attributes_lemmatized.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"adj\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), # need this to read list columns from csv\n",
    "        \"active\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "        \"patient\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "        }\n",
    "    )\n",
    "characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sort_meaningful(characters, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters from tv_tropes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tropes_list=[]\n",
    "with open('data/MovieSummaries/tvtropes.clusters.txt', 'r') as f:\n",
    "    s = f.readline()\n",
    "    while s:\n",
    "        trope = s[:s.index('\\t')]\n",
    "        character = json.loads(s[s.index('\\t'): ])\n",
    "        character['trope'] = trope\n",
    "        tropes_list.append(character)\n",
    "        s = f.readline()\n",
    "topres_df = pd.DataFrame(tropes_list)\n",
    "topres_df.head()\n",
    "\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    'data/MovieSummaries/movie.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'title', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']\n",
    ")\n",
    "topres_df = topres_df.merge(movies, how='left', left_on='movie', right_on='title')[['char', 'movie', 'trope', 'wiki_id']]\n",
    "topres_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tropes_and_clusters = topres_df.merge(characters, how='left', left_on='wiki_id', right_on='wiki_id').dropna()\n",
    "tropes_and_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_name(names1, names2):\n",
    "    names1 = names1.values\n",
    "    names2 = names2.values\n",
    "    flag = []\n",
    "    for i in range(len(names1)):\n",
    "        flag.append(names2[i] in names1[i])\n",
    "    return flag\n",
    "\n",
    "tropes_and_clusters = tropes_and_clusters[same_name(tropes_and_clusters['char'], tropes_and_clusters['character'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_to_check = tropes_and_clusters[['character', 'adj', 'active', 'patient', 'trope', 'wiki_id', 'movie']].reset_index(drop=True)\n",
    "characters_to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering evaluation\n",
    "\n",
    "We want to see, how good our clustering algorithm performs compared to the original alorithm proposed in the paper [Learning Latent Personas of Film Characters](http://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def group_labels_by_clusters(clusters):\n",
    "    _, clusters = np.unique(clusters, return_inverse=True)\n",
    "    l = [[] for _ in range(np.max(clusters) + 1)]\n",
    "    for i in range(len(clusters)):\n",
    "        l[clusters[i]].append(i)\n",
    "    return l\n",
    "\n",
    "def variation_of_information(X, Y):\n",
    "    n = float(sum([len(x) for x in X]))\n",
    "    sigma = 0.0\n",
    "    for x in X:\n",
    "        p = len(x) / n\n",
    "        for y in Y:\n",
    "           q = len(y) / n\n",
    "           r = len(set(x) & set(y)) / n\n",
    "           if r > 0.0:\n",
    "               sigma += r * (log(r / p, 2) + log(r / q, 2))\n",
    "    return abs(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_tropes = group_labels_by_clusters(characters_to_check['trope'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tv_tropes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative_clusters_n = [25, 50, 100]\n",
    "n_components = [25, 50, 100]\n",
    "\n",
    "configs = {}\n",
    "config_base = {'characters': characters_to_check, 'min_freq': 3, 'max_freq':1.0}\n",
    "\n",
    "for alg_n in agglomerative_clusters_n:\n",
    "    for n in n_components:\n",
    "        config = config_base.copy()\n",
    "        config['clustering_algo'] = AgglomerativeClustering(n_clusters=alg_n, metric='cosine', linkage='complete')\n",
    "        config['n_components'] = n\n",
    "        configs[f'{alg_n} topics, {n} archetypes'] = config\n",
    "\n",
    "results_lda = {}\n",
    "for k, config in configs.items():\n",
    "    clusters = get_lda_clusters(**config)\n",
    "    results_lda[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes)\n",
    "    print(k, f'VI = {results_lda[k]}')\n",
    "\n",
    "clear_output(wait=True)\n",
    "results_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the results are even better (K=100, P=100, 5.42 in the paper and 4.68 here) than the results from the [paper](http://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf). That could indicate that using word2vec embeddings and Agglomerative clustering of the words to topics might be better suited for dividing the words into topics for the purpose of personas extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_with_trf_emb =  pd.read_csv(\n",
    "    'data/trf_embeddings_for_labeled_characters.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"emb\": lambda x: [float(k) for k in x.strip(\"[]\").replace(\"'\",\"\").split(\", \")]\n",
    "        }\n",
    "    )\n",
    "characters_with_trf_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tropes_and_clusters = characters_to_check.merge(characters_with_trf_emb, how='left', left_on='wiki_id', right_on='wiki_id').dropna()\n",
    "tropes_and_clusters = tropes_and_clusters[tropes_and_clusters['character_x'] == tropes_and_clusters['character_y']]\n",
    "characters_to_check_trf = tropes_and_clusters[['character_y', 'emb', 'trope', 'wiki_id', 'movie']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_tropes_trf = group_labels_by_clusters(characters_to_check_trf['trope'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_trf = {}\n",
    "for n in n_components:\n",
    "    k = f'{n} archetypes, agglomerative clustering'\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n, metric='euclidean', linkage='complete')\n",
    "    clusters = get_trf_clusters(characters_to_check_trf, agglomerative)\n",
    "    results_trf[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes_trf)\n",
    "    print(k, f'VI = {results_trf[k]}')\n",
    "\n",
    "    k = f'{n} archetypes, kmeans clustering'\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    clusters = get_trf_clusters(characters_to_check_trf, kmeans)\n",
    "    results_trf[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes_trf)\n",
    "    print(k, f'VI = {results_trf[k]}')\n",
    "\n",
    "clear_output(wait=True)\n",
    "results_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still better than the results from the paper, but extracting BERT embeddings is very slow, so we will stick to the previous method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medoid(vec):\n",
    "    dist_matrix = np.zeros((len(vec), len(vec)))\n",
    "    for i in range(len(vec)):\n",
    "        for j in range(i + 1, len(vec)):\n",
    "            dist_matrix[i][j] = np.sum(np.abs(vec[i] - vec[j]))\n",
    "            dist_matrix[j][i] = dist_matrix[i][j]\n",
    "    argmin = np.argmin(np.sum(dist_matrix, axis=0))\n",
    "    return vec[argmin]\n",
    "\n",
    "def unsupervised_evaluation(features, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Calculate cluster medoid\n",
    "    cluster_medoids = np.array([medoid(features[labels == label]) for label in unique_labels])\n",
    "\n",
    "    # Calculate within-cluster sum of squares (WSS)\n",
    "    wss = 0\n",
    "    for num, label in enumerate(unique_labels):\n",
    "        distance = np.sum((features[labels == label] - cluster_medoids[num]) ** 2)\n",
    "        wss += distance\n",
    "\n",
    "    return wss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_for_eval = characters.sample(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algo = AgglomerativeClustering(n_clusters=100, metric='cosine', linkage='complete')\n",
    "\n",
    "k_values = np.arange(5, 36, 5)\n",
    "wsss = []\n",
    "min_wss_idx = 0\n",
    "\n",
    "for i in tqdm(range(len(k_values))):\n",
    "    y, X = get_lda_clusters(characters_for_eval, 3, 0.9, clustering_algo, k_values[i], return_topic_counts=True)\n",
    "    wss = unsupervised_evaluation(X, y)\n",
    "    wsss.append(wss)\n",
    "    if wsss[min_wss_idx] > wss:\n",
    "        min_wss_idx = i\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, wsss)\n",
    "plt.title(\"WSS scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "clusters = get_lda_clusters(characters, 3, 0.9, clustering_algo, k_values[min_wss_idx])\n",
    "characters['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "characters.to_csv('data/character_clusters.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
