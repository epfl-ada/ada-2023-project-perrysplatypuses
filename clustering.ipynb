{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:32.361200Z",
     "iopub.status.busy": "2023-11-16T21:37:32.361001Z",
     "iopub.status.idle": "2023-11-16T21:37:32.376251Z",
     "shell.execute_reply": "2023-11-16T21:37:32.375646Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:32.379173Z",
     "iopub.status.busy": "2023-11-16T21:37:32.378661Z",
     "iopub.status.idle": "2023-11-16T21:37:33.070102Z",
     "shell.execute_reply": "2023-11-16T21:37:33.069377Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:33.073192Z",
     "iopub.status.busy": "2023-11-16T21:37:33.072898Z",
     "iopub.status.idle": "2023-11-16T21:37:33.085376Z",
     "shell.execute_reply": "2023-11-16T21:37:33.084838Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:33.087845Z",
     "iopub.status.busy": "2023-11-16T21:37:33.087469Z",
     "iopub.status.idle": "2023-11-16T21:37:33.463811Z",
     "shell.execute_reply": "2023-11-16T21:37:33.463000Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clustering import get_lda_clusters, get_vocab, word_topics_clustering, sort_meaningful, get_trf_clusters, topic_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:33.467374Z",
     "iopub.status.busy": "2023-11-16T21:37:33.466916Z",
     "iopub.status.idle": "2023-11-16T21:37:36.763752Z",
     "shell.execute_reply": "2023-11-16T21:37:36.763021Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.clustering import get_lda_clusters, get_vocab, word_topics_clustering, sort_meaningful, get_trf_clusters, topic_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:36.766883Z",
     "iopub.status.busy": "2023-11-16T21:37:36.766600Z",
     "iopub.status.idle": "2023-11-16T21:37:37.867718Z",
     "shell.execute_reply": "2023-11-16T21:37:37.867048Z"
    }
   },
   "outputs": [],
   "source": [
    "characters =  pd.read_csv(\n",
    "    'data/character_attributes.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"adj\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), # need this to read list columns from csv\n",
    "        \"active\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "        \"patient\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "        }\n",
    "    )\n",
    "characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sort_meaningful(characters, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters from tv_tropes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:40.324657Z",
     "iopub.status.busy": "2023-11-16T21:37:40.324460Z",
     "iopub.status.idle": "2023-11-16T21:37:40.582686Z",
     "shell.execute_reply": "2023-11-16T21:37:40.581959Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tropes_list=[]\n",
    "with open('data/MovieSummaries/tvtropes.clusters.txt', 'r') as f:\n",
    "    s = f.readline()\n",
    "    while s:\n",
    "        trope = s[:s.index('\\t')]\n",
    "        character = json.loads(s[s.index('\\t'): ])\n",
    "        character['trope'] = trope\n",
    "        tropes_list.append(character)\n",
    "        s = f.readline()\n",
    "topres_df = pd.DataFrame(tropes_list)\n",
    "topres_df.head()\n",
    "\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    'data/MovieSummaries/movie.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'title', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']\n",
    ")\n",
    "topres_df = topres_df.merge(movies, how='left', left_on='movie', right_on='title')[['char', 'movie', 'trope', 'wiki_id']]\n",
    "topres_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:40.585373Z",
     "iopub.status.busy": "2023-11-16T21:37:40.585160Z",
     "iopub.status.idle": "2023-11-16T21:37:40.644525Z",
     "shell.execute_reply": "2023-11-16T21:37:40.643796Z"
    }
   },
   "outputs": [],
   "source": [
    "tropes_and_clusters = topres_df.merge(characters, how='left', left_on='wiki_id', right_on='wiki_id').dropna()\n",
    "tropes_and_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:40.647049Z",
     "iopub.status.busy": "2023-11-16T21:37:40.646843Z",
     "iopub.status.idle": "2023-11-16T21:37:40.676615Z",
     "shell.execute_reply": "2023-11-16T21:37:40.676036Z"
    }
   },
   "outputs": [],
   "source": [
    "def same_name(names1, names2):\n",
    "    names1 = names1.values\n",
    "    names2 = names2.values\n",
    "    flag = []\n",
    "    for i in range(len(names1)):\n",
    "        flag.append(names2[i] in names1[i])\n",
    "    return flag\n",
    "\n",
    "tropes_and_clusters = tropes_and_clusters[same_name(tropes_and_clusters['char'], tropes_and_clusters['character'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:40.679081Z",
     "iopub.status.busy": "2023-11-16T21:37:40.678889Z",
     "iopub.status.idle": "2023-11-16T21:37:40.720421Z",
     "shell.execute_reply": "2023-11-16T21:37:40.719766Z"
    }
   },
   "outputs": [],
   "source": [
    "characters_to_check = tropes_and_clusters[['character', 'adj', 'active', 'patient', 'trope', 'wiki_id', 'movie']].reset_index(drop=True)\n",
    "characters_to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering evaluation\n",
    "\n",
    "We want to see, how good our clustering algorithm performs compared to the original alorithm proposed in the paper [Learning Latent Personas of Film Characters](http://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:40.723138Z",
     "iopub.status.busy": "2023-11-16T21:37:40.722939Z",
     "iopub.status.idle": "2023-11-16T21:37:40.753749Z",
     "shell.execute_reply": "2023-11-16T21:37:40.753172Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def group_labels_by_clusters(clusters):\n",
    "    _, clusters = np.unique(clusters, return_inverse=True)\n",
    "    l = [[] for _ in range(np.max(clusters) + 1)]\n",
    "    for i in range(len(clusters)):\n",
    "        l[clusters[i]].append(i)\n",
    "    return l\n",
    "\n",
    "def variation_of_information(X, Y):\n",
    "    n = float(sum([len(x) for x in X]))\n",
    "    sigma = 0.0\n",
    "    for x in X:\n",
    "        p = len(x) / n\n",
    "        for y in Y:\n",
    "           q = len(y) / n\n",
    "           r = len(set(x) & set(y)) / n\n",
    "           if r > 0.0:\n",
    "               sigma += r * (log(r / p, 2) + log(r / q, 2))\n",
    "    return abs(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:40.756592Z",
     "iopub.status.busy": "2023-11-16T21:37:40.756393Z",
     "iopub.status.idle": "2023-11-16T21:37:40.792437Z",
     "shell.execute_reply": "2023-11-16T21:37:40.791892Z"
    }
   },
   "outputs": [],
   "source": [
    "tv_tropes = group_labels_by_clusters(characters_to_check['trope'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:40.794914Z",
     "iopub.status.busy": "2023-11-16T21:37:40.794717Z",
     "iopub.status.idle": "2023-11-16T21:37:40.825841Z",
     "shell.execute_reply": "2023-11-16T21:37:40.825208Z"
    }
   },
   "outputs": [],
   "source": [
    "len(tv_tropes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:40.828500Z",
     "iopub.status.busy": "2023-11-16T21:37:40.828120Z",
     "iopub.status.idle": "2023-11-16T21:37:53.966180Z",
     "shell.execute_reply": "2023-11-16T21:37:53.965481Z"
    }
   },
   "outputs": [],
   "source": [
    "agglomerative_clusters_n = [25, 50, 100]\n",
    "n_components = [25, 50, 100]\n",
    "\n",
    "configs = {}\n",
    "config_base = {'characters': characters_to_check, 'min_freq': 5, 'max_freq':0.9}\n",
    "\n",
    "for alg_n in agglomerative_clusters_n:\n",
    "    for n in n_components:\n",
    "        config = config_base.copy()\n",
    "        config['clustering_algo'] = AgglomerativeClustering(n_clusters=alg_n, metric='cosine', linkage='complete')\n",
    "        config['n_components'] = n\n",
    "        configs[f'{alg_n} topics, {n} archetypes'] = config\n",
    "\n",
    "results_lda = {}\n",
    "for k, config in configs.items():\n",
    "    clusters = get_lda_clusters(**config)\n",
    "    results_lda[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes)\n",
    "    print(k, f'VI = {results_lda[k]}')\n",
    "\n",
    "clear_output(wait=True)\n",
    "results_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the results are even better (K=100, P=100, 5.42 in the paper and 4.9 here) than the results from the [paper](http://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf). That could indicate that using word2vec embeddings and Agglomerative clustering of the words to topics might be better suited for dividing the words into topics for the purpose of personas extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:53.969636Z",
     "iopub.status.busy": "2023-11-16T21:37:53.969021Z",
     "iopub.status.idle": "2023-11-16T21:37:55.393253Z",
     "shell.execute_reply": "2023-11-16T21:37:55.392580Z"
    }
   },
   "outputs": [],
   "source": [
    "characters_with_trf_emb =  pd.read_csv(\n",
    "    'data/trf_embeddings_for_labeled_characters.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"emb\": lambda x: [float(k) for k in x.strip(\"[]\").replace(\"'\",\"\").split(\", \")]\n",
    "        }\n",
    "    )\n",
    "characters_with_trf_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:55.396086Z",
     "iopub.status.busy": "2023-11-16T21:37:55.395618Z",
     "iopub.status.idle": "2023-11-16T21:37:55.431831Z",
     "shell.execute_reply": "2023-11-16T21:37:55.431279Z"
    }
   },
   "outputs": [],
   "source": [
    "tropes_and_clusters = characters_to_check.merge(characters_with_trf_emb, how='left', left_on='wiki_id', right_on='wiki_id').dropna()\n",
    "tropes_and_clusters = tropes_and_clusters[tropes_and_clusters['character_x'] == tropes_and_clusters['character_y']]\n",
    "characters_to_check_trf = tropes_and_clusters[['character_y', 'emb', 'trope', 'wiki_id', 'movie']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:55.434639Z",
     "iopub.status.busy": "2023-11-16T21:37:55.434248Z",
     "iopub.status.idle": "2023-11-16T21:37:55.462187Z",
     "shell.execute_reply": "2023-11-16T21:37:55.461643Z"
    }
   },
   "outputs": [],
   "source": [
    "tv_tropes_trf = group_labels_by_clusters(characters_to_check_trf['trope'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-16T21:37:55.464933Z",
     "iopub.status.busy": "2023-11-16T21:37:55.464518Z",
     "iopub.status.idle": "2023-11-16T21:37:57.370391Z",
     "shell.execute_reply": "2023-11-16T21:37:57.369642Z"
    }
   },
   "outputs": [],
   "source": [
    "results_trf = {}\n",
    "for n in n_components:\n",
    "    k = f'{n} archetypes, agglomerative clustering'\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n, metric='euclidean', linkage='complete')\n",
    "    clusters = get_trf_clusters(characters_to_check_trf, agglomerative)\n",
    "    results_trf[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes_trf)\n",
    "    print(k, f'VI = {results_trf[k]}')\n",
    "\n",
    "    k = f'{n} archetypes, kmeans clustering'\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    clusters = get_trf_clusters(characters_to_check_trf, kmeans)\n",
    "    results_trf[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes_trf)\n",
    "    print(k, f'VI = {results_trf[k]}')\n",
    "\n",
    "clear_output(wait=True)\n",
    "results_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still better than the results from the paper, but extracting BERT embeddings is very slow, so we will stick to the previous method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medoid(vec):\n",
    "    dist_matrix = np.zeros((len(vec), len(vec)))\n",
    "    for i in range(len(vec)):\n",
    "        for j in range(i + 1, len(vec)):\n",
    "            dist_matrix[i][j] = np.sum(np.abs(vec[i] - vec[j]))\n",
    "            dist_matrix[j][i] = dist_matrix[i][j]\n",
    "    argmin = np.argmin(np.sum(dist_matrix, axis=0))\n",
    "    return vec[argmin]\n",
    "\n",
    "def unsupervised_evaluation(features, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Calculate cluster medoid\n",
    "    cluster_medoids = np.array([medoid(features[labels == label]) for label in unique_labels])\n",
    "\n",
    "    # Calculate within-cluster sum of squares (WSS)\n",
    "    wss = 0\n",
    "    for num, label in enumerate(unique_labels):\n",
    "        distance = np.sum((features[labels == label] - cluster_medoids[num]) ** 2)\n",
    "        wss += distance\n",
    "\n",
    "    sil_score = metrics.silhouette_score(features, labels)\n",
    "    return wss, sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_for_eval = characters.sample(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algo = AgglomerativeClustering(n_clusters=200, metric='cosine', linkage='complete')\n",
    "\n",
    "k_values = np.arange(10, 101, 5)\n",
    "wsss = []\n",
    "silhouettes = []\n",
    "min_wss_idx = 0\n",
    "\n",
    "for i in tqdm(range(len(k_values))):\n",
    "    y, X = get_lda_clusters(characters_for_eval, 5, 0.9, clustering_algo, k_values[i], return_topic_counts=True)\n",
    "    wss, silhouette = unsupervised_evaluation(X, y)\n",
    "    wsss.append(wss)\n",
    "    silhouettes.append(silhouette)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, wsss)\n",
    "plt.title(\"WSS scores\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, silhouettes)\n",
    "plt.title(\"Silhouette scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that the 60 clusters provide relatively good combination of the silhouette and WSS scores and is still manageable to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# We want to save components of lda and word clusters (topic_dict) as well as clustering\n",
    "\n",
    "vocab, vocab_vectors = get_vocab(characters, 5, 0.9)\n",
    "topic_dict = word_topics_clustering(vocab, vocab_vectors, clustering_algo)\n",
    "counts = topic_count(characters, topic_dict)\n",
    "lda = LatentDirichletAllocation(\n",
    "        n_components=optimal_k, random_state=0\n",
    ").fit(counts)\n",
    "\n",
    "characters['cluster'] = lda.transform(counts).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "characters.to_csv('data/character_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "topic_to_words = {}\n",
    "\n",
    "for k, v in topic_dict.items():\n",
    "    v = str(v)\n",
    "    if v in topic_to_words:\n",
    "        topic_to_words[v].append(k)\n",
    "    else:\n",
    "        topic_to_words[v] = [k]\n",
    "        \n",
    "json.dump( topic_to_words, open( \"data/words_by_topic.json\", 'w' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "with open('data/lda_components.npy', 'wb') as f:\n",
    "    np.save(f, lda.components_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
