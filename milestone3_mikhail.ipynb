{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imdb_ratings import movies_with_imdb_rating\n",
    "from utils.cluster_interpretation import plot_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What isn't included in this notebook\n",
    "\n",
    "This project required a lot of preprocessing, which is an interesting task, but is not related to the research questions. In this notebook we will focus on the research questions only.\n",
    "\n",
    "For extracting characters and their attributes from the plot texts, refer to `extract_character_attributes.ipynb`.\n",
    "\n",
    "For the clustering method please refer to `clustering.ipynb`, there you can find the methods comparison and the pipeline for characters clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters =  pd.read_csv(\n",
    "    'data/character_clusters.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"adj\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), # need this to read list columns from csv\n",
    "        \"active\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "        \"patient\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "        }\n",
    "    )\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    'data/MovieSummaries/movie.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'title', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']\n",
    ")\n",
    "\n",
    "actors = pd.read_csv(\n",
    "    'data/MovieSummaries/character.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'release_date', 'character', 'date_of_birth', 'sex', 'height', '.','actor','age','character_map','..','...','....']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_data = pd.read_csv('data/cpi_data.csv', )\n",
    "cpi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_name(names1, names2):\n",
    "    names1 = names1.values\n",
    "    names2 = names2.values\n",
    "    flag = []\n",
    "    for i in range(len(names1)):\n",
    "        flag.append(names1[i] in names2[i])\n",
    "    return flag\n",
    "\n",
    "\n",
    "actors_and_characters = characters.merge(actors, how='left', left_on='wiki_id', right_on='wiki_id').dropna(subset=['character_y'])\n",
    "\n",
    "actors_and_characters = actors_and_characters[same_name(actors_and_characters['character_x'], actors_and_characters['character_y'])]\n",
    "actors_and_characters['character'] = actors_and_characters['character_x']\n",
    "actors_and_characters = actors_and_characters.drop(columns=['character_x', 'character_y'])\n",
    "actors_and_characters = actors_and_characters[['character', 'actor', 'cluster', 'wiki_id', 'release_date', 'date_of_birth', 'sex', 'height', 'age', 'adj', 'active', 'patient']]\n",
    "actors_and_characters.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_revenue(year, revenue):\n",
    "    if year in cpi_data['year']:\n",
    "        cpi = cpi_data[cpi_data['year'] == year]['cpi'][0]\n",
    "    else:\n",
    "        cpi = 1\n",
    "    return (revenue /  cpi)*100\n",
    "\n",
    "\n",
    "map_dict_to_list = lambda x: [value for key, value in eval(x).items()]\n",
    "release_year = lambda x: pd.to_numeric(x.str.replace(r'-\\d{2}-\\d{2}$', '', regex=True).str.replace(r'-\\d{2}$', '', regex=True))\n",
    "\n",
    "movies['languages'] = movies['languages'].apply(map_dict_to_list)\n",
    "movies['countries'] = movies['countries'].apply(map_dict_to_list)\n",
    "movies['genres'] = movies['genres'].apply(map_dict_to_list)\n",
    "\n",
    "movies[\"release_year\"] = release_year(movies['release_date'])\n",
    "movies[\"release_year\"] = movies['release_year'].apply(lambda x: x if x > 1800 else x + 1000)\n",
    "\n",
    "movies['discounted_revenue'] = movies.apply(lambda x: discount_revenue(x.release_year, x.revenue), axis=1)\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Script takes time to run, so we will use saved version instead\n",
    "movies_with_rating = movies_with_imdb_rating(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_with_rating = pd.read_csv(\n",
    "    'data/movies_with_rating.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"languages\": map_dict_to_list,\n",
    "        \"countries\": map_dict_to_list,\n",
    "        \"genres\": map_dict_to_list\n",
    "        }\n",
    "    )\n",
    "    \n",
    "movies_with_rating['release_year'] = release_year(movies_with_rating['release_date'])\n",
    "movies_with_rating['discounted_revenue'] = movies_with_rating.apply(lambda x: discount_revenue(x.release_year, x.revenue), axis=1)\n",
    "\n",
    "movies_with_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of movies: {len(movies)}\")\n",
    "print(f\"Number of movies with revenue: {movies['revenue'].notna().sum()}\")\n",
    "print(f\"Number of movies with rating: {len(movies_with_rating)}\")\n",
    "print(f\"Number of movies with rating and revenue: {movies_with_rating['revenue'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = {\n",
    "    \"All movies\": len(movies),\n",
    "    \"With rating\": len(movies_with_rating),\n",
    "    \"With revenue\": movies['revenue'].notna().sum(),\n",
    "    \"With rating and revenue\": movies_with_rating['revenue'].notna().sum()\n",
    "}\n",
    "\n",
    "labels = list(data.keys())\n",
    "values = list(data.values())\n",
    "\n",
    "plt.bar(labels, values, color = ['blue', 'orange', 'red', 'green'])\n",
    "# plt.xlabel('Categories')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.title('Movie Information')\n",
    "plt.xticks(rotation=45, ha='center')\n",
    "\n",
    "\n",
    "# Display the values on top of the bars\n",
    "for i, value in enumerate(values):\n",
    "    plt.text(i, value + 0.1, str(value), ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = pd.read_csv(\n",
    "    'data/MovieSummaries/plot_summaries.txt', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'plot']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_and_plots = movies.merge(plots, how='right', left_on='wiki_id', right_on='wiki_id')\n",
    "num_plot = len(pd.unique(movies_and_plots['wiki_id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_and_characters = movies.merge(characters[['wiki_id', 'character']], how='right', left_on='wiki_id', right_on='wiki_id')\n",
    "movies_and_characters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_char = len(pd.unique(movies_and_characters['wiki_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of movies: {len(movies)}\")\n",
    "print(f\"Number of movies with revenue: {movies['revenue'].notna().sum()}\")\n",
    "print(f\"Number of movies with rating: {len(movies_with_rating)}\")\n",
    "print(f\"Number of movies with rating and revenue: {movies_with_rating['revenue'].notna().sum()}\")\n",
    "print(f\"Number of movies with plot: {num_plot}\")\n",
    "print(f\"Number of movies, where we find archetypes: {num_char}\")\n",
    "print(f\"Number of actors with the characters who have an archetype: {len(actors_and_characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of characters with archetypes: {len(characters)}\")\n",
    "print(f\"Number of actors: {len(actors)}\")\n",
    "print(f\"Number of actors with the characters who have an archetype: {len(actors_and_characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of actors with the characters who have an archetype in the movies with revenue and rating: {len(actors_and_characters[actors_and_characters['wiki_id'].isin(movies_with_rating[movies_with_rating['revenue'].notna()]['wiki_id'])])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coutries_distr = movies.explode('countries').groupby('countries').size()\n",
    "coutries_distr_with_rating = movies_with_rating.explode('countries').groupby('countries').size()\n",
    "coutries_distr_with_rating_and_revenue = movies_with_rating[movies_with_rating['revenue'].notna()].explode('countries').groupby('countries').size()\n",
    "\n",
    "coutries = list(set(\n",
    "    coutries_distr.sort_values(ascending=False)[:20].index.to_list() \n",
    "    + coutries_distr_with_rating.sort_values(ascending=False)[:20].index.to_list() \n",
    "    + coutries_distr_with_rating_and_revenue.sort_values(ascending=False)[:20].index.to_list()))\n",
    "\n",
    "coutries_distr = coutries_distr.loc[coutries].sort_values(ascending=True)\n",
    "coutries = coutries_distr.index.to_list() \n",
    "coutries_distr_with_rating = coutries_distr_with_rating.loc[coutries]\n",
    "coutries_distr_with_rating_and_revenue = coutries_distr_with_rating_and_revenue.loc[coutries]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title('Top of movie production countries')\n",
    "\n",
    "plt.barh(coutries_distr.index, coutries_distr.values, label='all movies')\n",
    "plt.barh(coutries_distr_with_rating.index, coutries_distr_with_rating.values, label='movies with rating')\n",
    "plt.barh(coutries_distr_with_rating_and_revenue.index, coutries_distr_with_rating_and_revenue.values, label='movies with rating and revenue')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that most of the movies in the dataset are made in the US, moreover, we have much less data for movies with revenue and this data is't distributed prportionally to the overall number of movies produced in the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.groupby('release_year').size().plot(figsize=(15, 5), title='Number of released movies', label='number of released movies')\n",
    "plt.xticks(np.arange(1890, 2021, 7))\n",
    "\n",
    "plt.axvspan(1914, 1918, alpha=0.3, label='World War I')\n",
    "plt.axvspan(1929, 1939, alpha=0.3, label='Great Depression', color='green')\n",
    "plt.axvspan(1939, 1945, alpha=0.3, label='World War II')\n",
    "plt.axvspan(1961.2, 1961.3, alpha=0.3, label='First space flight', color='purple')\n",
    "plt.axvspan(2007, 2008, alpha=0.3, label='Global Financial Crisis', color='green')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have much data before 1910-s and after 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters interpretation\n",
    "To interpret clusters, we can use the function `plot_topic_distribution` to see the topics with the largest probabilities to be in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historycal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_and_movies = characters.merge(movies, left_on='wiki_id', right_on='wiki_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_count = characters_and_movies.groupby('release_year').size().reset_index(name='movie_count')\n",
    "movies_count = movies_count[movies_count['movie_count'] >= 15]\n",
    "movies_count.plot(x='release_year', y='movie_count')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: We decide to analyze trends where there is a stable abundance of data, and remove movies before 1932 and tha last two years (2013-2014). For further analysis we are selecting important clusters (by relative popularity or changes in popularity) but this selection is skewed by the years where there is little data since that gives a very high proportion for every cluster. So the early clusters will appear very significant despite that not being the case (if e.g. there are only a handful of movies, the archetype distribution is not very interesting). Therefore the filtered subset is used, not only for plot, but also for cluster ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_counts = characters_and_movies[characters_and_movies['release_year'].isin(movies_count['release_year'])].groupby(['release_year', 'cluster']).size().reset_index(name='character_count')\n",
    "archetype_counts = archetype_counts.pivot(index='release_year', columns='cluster', values='character_count').fillna(0)\n",
    "archetype_counts.plot(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_archetype_counts = (archetype_counts)/(archetype_counts.values.sum(1).reshape(-1, 1))\n",
    "normalized_archetype_counts.plot(legend=False)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top archetypes\n",
    "\n",
    "- By the highest sum of normalized frequency (popularity)\n",
    "- By the biggest range in normalized frequency (changes in popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of normalized frequency\n",
    "\n",
    "top_clusters = normalized_archetype_counts.sum(0).sort_values(ascending=False)[:10].index.values\n",
    "top_clusters_archetype_counts = normalized_archetype_counts[top_clusters]\n",
    "top_clusters_archetype_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clusters_archetype_counts.plot(figsize=(12, 6))\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title('Normalized character counts by cluster: subset 1')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "n = 10 # sliding average window size\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over clusters and plot a line for each\n",
    "for cluster in top_clusters:\n",
    "    x = top_clusters_archetype_counts[cluster]\n",
    "    x_avg = np.convolve(x, np.ones(n)/n, mode='valid')\n",
    "    y = top_clusters_archetype_counts.index\n",
    "    y_1 = y[round(n/2):-(n-round(n/2))+1]\n",
    "    plt.plot(y_1, x_avg, label=f'Cluster {cluster}', marker='', linewidth=0.7)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title(f'Normalized character counts by cluster: subset 1. Sliding average (n={n})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_info(n):\n",
    "    print('Cluster: ', n)\n",
    "    top = characters_and_movies[(characters_and_movies['cluster'] == n) & (characters_and_movies['revenue'] > 5e8)]\n",
    "    top = top.sort_values(by='revenue', ascending=False).head(5)\n",
    "    print(top[['title', 'character']])\n",
    "    plot_topic_distribution(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cluster_info(19)\n",
    "print_cluster_info(12)\n",
    "print_cluster_info(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biggest range in normalized frequency\n",
    "\n",
    "top_clusters = normalized_archetype_counts.apply(np.ptp).sort_values(ascending=False)[:10].index.values\n",
    "top_clusters_archetype_counts = normalized_archetype_counts[top_clusters]\n",
    "top_clusters_archetype_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clusters_archetype_counts.plot(figsize=(12, 6))\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title('Normalized character counts by cluster: subset 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "n = 10 # sliding average window size\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over clusters and plot a line for each\n",
    "for cluster in top_clusters:\n",
    "    x = top_clusters_archetype_counts[cluster]\n",
    "    x_avg = np.convolve(x, np.ones(n)/n, mode='valid')\n",
    "    y = top_clusters_archetype_counts.index\n",
    "    y_1 = y[round(n/2):-(n-round(n/2))+1]\n",
    "    plt.plot(y_1, x_avg, label=f'Cluster {cluster}', marker='', linewidth=0.7)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title(f'Normalized character counts by cluster: subset 1. Sliding average (n={n})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cluster_info(10)\n",
    "print_cluster_info(5)\n",
    "print_cluster_info(38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cultural preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_and_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_movies_countries = characters_and_movies[characters_and_movies['release_year'].isin(movies_count['release_year'])].explode('countries').groupby(['countries', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_by_country = char_movies_countries.size().reset_index(name='character_count')\n",
    "archetype_by_country = archetype_by_country.pivot(index='countries', columns='cluster', values='character_count').fillna(0)\n",
    "archetype_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_by_country.sum(1).sort_values(ascending=False)[:50].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have Germany, West Germany, Weimar Republic, German Democratic Republic - let's merge them\n",
    "archetype_by_country.loc['Germany'] = archetype_by_country.loc[['Germany', 'West Germany', 'Weimar Republic', 'German Democratic Republic']].sum()\n",
    "\n",
    "archetype_by_country = archetype_by_country.drop(['West Germany', 'Weimar Republic', 'German Democratic Republic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_by_country.sum(1).sort_values(ascending=False)[:10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_archetype_by_country = (archetype_by_country)/(archetype_by_country.values.sum(1).reshape(-1, 1))\n",
    "normalized_archetype_by_country.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "colormap = plt.cm.summer\n",
    "\n",
    "\n",
    "\n",
    "countries = archetype_by_country.sum(1).sort_values(ascending=False)[:5].index\n",
    "\n",
    "for country in countries:\n",
    "\n",
    "\n",
    "    top_country_arch = normalized_archetype_by_country.loc[country].sort_values(ascending=False)[:10]\n",
    "    normalize = Normalize(vmin=top_country_arch.min(), vmax=top_country_arch.max())\n",
    "\n",
    "    top_country_arch[::-1].plot(kind='barh', color=colormap(normalize(top_country_arch)))\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.title(country)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean = [1/len(normalized_archetype_by_country.columns)] * len(normalized_archetype_by_country.columns)\n",
    "# mean = np.array(mean)\n",
    "# mean.shape\n",
    "# information_heterogeneity = normalized_archetype_by_country[country] - mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie success based on the archetypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors success based on the archetypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
