{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imdb_ratings import movies_with_imdb_rating\n",
    "from utils.cluster_interpretation import plot_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What isn't included in this notebook\n",
    "\n",
    "This project required a lot of preprocessing, which is an interesting task, but is not related to the research questions. In this notebook we will focus on the research questions only.\n",
    "\n",
    "For extracting characters and their attributes from the plot texts, refer to `extract_character_attributes.ipynb`.\n",
    "\n",
    "For the clustering method please refer to `clustering.ipynb`, there you can find the methods comparison and the pipeline for characters clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters =  pd.read_csv(\n",
    "    'data/character_clusters.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"adj\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), # need this to read list columns from csv\n",
    "        \"active\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "        \"patient\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "        }\n",
    "    )\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    'data/MovieSummaries/movie.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'title', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']\n",
    ")\n",
    "\n",
    "actors = pd.read_csv(\n",
    "    'data/MovieSummaries/character.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'release_date', 'character', 'date_of_birth', 'sex', 'height', '.','actor','age','character_map','..','...','....']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_data = pd.read_csv('data/cpi_data.csv', )\n",
    "cpi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_name(names1, names2):\n",
    "    names1 = names1.values\n",
    "    names2 = names2.values\n",
    "    flag = []\n",
    "    for i in range(len(names1)):\n",
    "        flag.append(names1[i] in names2[i])\n",
    "    return flag\n",
    "\n",
    "\n",
    "actors_and_characters = characters.merge(actors, how='left', left_on='wiki_id', right_on='wiki_id').dropna(subset=['character_y'])\n",
    "\n",
    "actors_and_characters = actors_and_characters[same_name(actors_and_characters['character_x'], actors_and_characters['character_y'])]\n",
    "actors_and_characters['character'] = actors_and_characters['character_x']\n",
    "actors_and_characters = actors_and_characters.drop(columns=['character_x', 'character_y'])\n",
    "actors_and_characters = actors_and_characters[['character', 'actor', 'cluster', 'wiki_id', 'release_date', 'date_of_birth', 'sex', 'height', 'age', 'adj', 'active', 'patient']]\n",
    "actors_and_characters.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_revenue(year, revenue):\n",
    "    if year in cpi_data['year']:\n",
    "        cpi = cpi_data[cpi_data['year'] == year]['cpi'][0]\n",
    "    else:\n",
    "        cpi = 1\n",
    "    return (revenue /  cpi)*100\n",
    "\n",
    "\n",
    "map_dict_to_list = lambda x: [value for key, value in eval(x).items()]\n",
    "release_year = lambda x: pd.to_numeric(x.str.replace(r'-\\d{2}-\\d{2}$', '', regex=True).str.replace(r'-\\d{2}$', '', regex=True))\n",
    "\n",
    "movies['languages'] = movies['languages'].apply(map_dict_to_list)\n",
    "movies['countries'] = movies['countries'].apply(map_dict_to_list)\n",
    "movies['genres'] = movies['genres'].apply(map_dict_to_list)\n",
    "\n",
    "movies[\"release_year\"] = release_year(movies['release_date'])\n",
    "movies[\"release_year\"] = movies['release_year'].apply(lambda x: x if x > 1800 else x + 1000)\n",
    "\n",
    "movies['discounted_revenue'] = movies.apply(lambda x: discount_revenue(x.release_year, x.revenue), axis=1)\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Script takes time to run, so we will use saved version instead\n",
    "movies_with_rating = movies_with_imdb_rating(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_with_rating = pd.read_csv(\n",
    "    'data/movies_with_rating.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"languages\": map_dict_to_list,\n",
    "        \"countries\": map_dict_to_list,\n",
    "        \"genres\": map_dict_to_list\n",
    "        }\n",
    "    )\n",
    "    \n",
    "movies_with_rating['release_year'] = release_year(movies_with_rating['release_date'])\n",
    "movies_with_rating['discounted_revenue'] = movies_with_rating.apply(lambda x: discount_revenue(x.release_year, x.revenue), axis=1)\n",
    "\n",
    "movies_with_rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of movies: {len(movies)}\")\n",
    "print(f\"Number of movies with revenue: {movies['revenue'].notna().sum()}\")\n",
    "print(f\"Number of movies with rating: {len(movies_with_rating)}\")\n",
    "print(f\"Number of movies with rating and revenue: {movies_with_rating['revenue'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of characters with archetypes: {len(characters)}\")\n",
    "print(f\"Number of actors: {len(actors)}\")\n",
    "print(f\"Number of actors with the characters who have an archetype: {len(actors_and_characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of actors with the characters who have an archetype in the movies with revenue and rating: {len(actors_and_characters[actors_and_characters['wiki_id'].isin(movies_with_rating[movies_with_rating['revenue'].notna()]['wiki_id'])])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coutries_distr = movies.explode('countries').groupby('countries').size()\n",
    "coutries_distr_with_rating = movies_with_rating.explode('countries').groupby('countries').size()\n",
    "coutries_distr_with_rating_and_revenue = movies_with_rating[movies_with_rating['revenue'].notna()].explode('countries').groupby('countries').size()\n",
    "\n",
    "coutries = list(set(\n",
    "    coutries_distr.sort_values(ascending=False)[:20].index.to_list() \n",
    "    + coutries_distr_with_rating.sort_values(ascending=False)[:20].index.to_list() \n",
    "    + coutries_distr_with_rating_and_revenue.sort_values(ascending=False)[:20].index.to_list()))\n",
    "\n",
    "coutries_distr = coutries_distr.loc[coutries].sort_values(ascending=True)\n",
    "coutries = coutries_distr.index.to_list() \n",
    "coutries_distr_with_rating = coutries_distr_with_rating.loc[coutries]\n",
    "coutries_distr_with_rating_and_revenue = coutries_distr_with_rating_and_revenue.loc[coutries]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title('Top of movie production countries')\n",
    "\n",
    "plt.barh(coutries_distr.index, coutries_distr.values, label='all movies')\n",
    "plt.barh(coutries_distr_with_rating.index, coutries_distr_with_rating.values, label='movies with rating')\n",
    "plt.barh(coutries_distr_with_rating_and_revenue.index, coutries_distr_with_rating_and_revenue.values, label='movies with rating and revenue')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that most of the movies in the dataset are made in the US, moreover, we have much less data for movies with revenue and this data is't distributed prportionally to the overall number of movies produced in the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.groupby('release_year').size().plot(figsize=(15, 5), title='Number of released movies', label='number of released movies')\n",
    "plt.xticks(np.arange(1890, 2021, 7))\n",
    "\n",
    "plt.axvspan(1914, 1918, alpha=0.3, label='World War I')\n",
    "plt.axvspan(1929, 1939, alpha=0.3, label='Great Depression', color='green')\n",
    "plt.axvspan(1939, 1945, alpha=0.3, label='World War II')\n",
    "plt.axvspan(1961.2, 1961.3, alpha=0.3, label='First space flight', color='purple')\n",
    "plt.axvspan(2007, 2008, alpha=0.3, label='Global Financial Crisis', color='green')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have much data before 1910-s and after 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters interpretation\n",
    "To interpret clusters, we can use the function `plot_topic_distribution` to see the topics with the largest probabilities to be in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historycal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_and_movies = characters.merge(movies, left_on='wiki_id', right_on='wiki_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_count = characters_and_movies.groupby('release_year').size().reset_index(name='movie_count')\n",
    "movies_count = movies_count[movies_count['movie_count'] >= 15]\n",
    "movies_count.plot(x='release_year', y='movie_count')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: We decide to analyze trends where there is a stable abundance of data, and remove movies before 1932 and tha last two years (2013-2014). For further analysis we are selecting important clusters (by relative popularity or changes in popularity) but this selection is skewed by the years where there is little data since that gives a very high proportion for every cluster. So the early clusters will appear very significant despite that not being the case (if e.g. there are only a handful of movies, the archetype distribution is not very interesting). Therefore the filtered subset is used, not only for plot, but also for cluster ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_counts = characters_and_movies[characters_and_movies['release_year'].isin(movies_count['release_year'])].groupby(['release_year', 'cluster']).size().reset_index(name='character_count')\n",
    "archetype_counts = archetype_counts.pivot(index='release_year', columns='cluster', values='character_count').fillna(0)\n",
    "archetype_counts.plot(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_archetype_counts = (archetype_counts)/(archetype_counts.values.sum(1).reshape(-1, 1))\n",
    "normalized_archetype_counts.plot(legend=False)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top archetypes\n",
    "\n",
    "- By the highest sum of normalized frequency (popularity)\n",
    "- By the biggest range in normalized frequency (changes in popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of normalized frequency\n",
    "\n",
    "top_clusters = normalized_archetype_counts.sum(0).sort_values(ascending=False)[:10].index.values\n",
    "top_clusters_archetype_counts = normalized_archetype_counts[top_clusters]\n",
    "top_clusters_archetype_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clusters_archetype_counts.plot(figsize=(12, 6))\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title('Normalized character counts by cluster: subset 1')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "n = 10 # sliding average window size\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over clusters and plot a line for each\n",
    "for cluster in top_clusters:\n",
    "    x = top_clusters_archetype_counts[cluster]\n",
    "    x_avg = np.convolve(x, np.ones(n)/n, mode='valid')\n",
    "    y = top_clusters_archetype_counts.index\n",
    "    y_1 = y[round(n/2):-(n-round(n/2))+1]\n",
    "    plt.plot(y_1, x_avg, label=f'Cluster {cluster}', marker='', linewidth=0.7)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title(f'Normalized character counts by cluster: subset 1. Sliding average (n={n})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_info(n):\n",
    "    print('Cluster: ', n)\n",
    "    top = characters_and_movies[(characters_and_movies['cluster'] == n) & (characters_and_movies['revenue'] > 5e8)]\n",
    "    top = top.sort_values(by='revenue', ascending=False).head(5)\n",
    "    print(top[['title', 'character']])\n",
    "    plot_topic_distribution(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cluster_info(19)\n",
    "print_cluster_info(12)\n",
    "print_cluster_info(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biggest range in normalized frequency\n",
    "\n",
    "top_clusters = normalized_archetype_counts.apply(np.ptp).sort_values(ascending=False)[:10].index.values\n",
    "top_clusters_archetype_counts = normalized_archetype_counts[top_clusters]\n",
    "top_clusters_archetype_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clusters_archetype_counts.plot(figsize=(12, 6))\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title('Normalized character counts by cluster: subset 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "n = 10 # sliding average window size\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over clusters and plot a line for each\n",
    "for cluster in top_clusters:\n",
    "    x = top_clusters_archetype_counts[cluster]\n",
    "    x_avg = np.convolve(x, np.ones(n)/n, mode='valid')\n",
    "    y = top_clusters_archetype_counts.index\n",
    "    y_1 = y[round(n/2):-(n-round(n/2))+1]\n",
    "    plt.plot(y_1, x_avg, label=f'Cluster {cluster}', marker='', linewidth=0.7)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title(f'Normalized character counts by cluster: subset 1. Sliding average (n={n})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cluster_info(10)\n",
    "print_cluster_info(5)\n",
    "print_cluster_info(38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cultural preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie success based on the archetypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors success based on the archetypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, We'll make a table of what archetypal characters each film contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"wiki_id\",\"Freebase movie ID\",\" Movie name\",\"Movie release date\", \"MovieBoxOfficeRevenue\",\"Movie runtime\",\"Movie languages\",\"Movie countries\",\"Movie genres\" ]\n",
    "df_movies = pd.read_csv(\"data/MovieSummaries/movie.metadata.tsv\", sep='\\t',names= colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_clusters = pd.read_csv(\"data/character_clusters.csv\")\n",
    "df_clusters = df_clusters.drop(df_clusters.columns[[0,3,4,5]], axis=1) # dropping unneccesary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging datasets\n",
    "df_merged = pd.merge(df_clusters, df_movies, on='wiki_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add column for each archetypes. And they will take binary number.\n",
    "for i in range(1,51):\n",
    "    df_merged['archetype{}'.format(i)]=df_merged['cluster'].map(lambda x: 1 if x== i else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the movie include the archetype[i], then the column archetype[i] will take 1 , otherwise 0.\n",
    "def dummy(s):\n",
    "    if sum(s)>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# this is just an opperation for getting Box office value after a groupby operation.\n",
    "def boxoffice(s):\n",
    "    return sum(s)/len(s)\n",
    "\n",
    "string= ''    \n",
    "for i in range(1,51):\n",
    "    string = string + '\\'archetype' +str(i) +'\\':dummy,'\n",
    "string = string[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moviearchetypes = df_merged.groupby('wiki_id').agg({'MovieBoxOfficeRevenue':boxoffice,'archetype1':dummy,'archetype2':dummy,'archetype3':dummy,'archetype4':dummy,'archetype5':dummy,'archetype6':dummy,'archetype7':dummy,'archetype8':dummy,'archetype9':dummy,'archetype10':dummy,'archetype11':dummy,'archetype12':dummy,'archetype13':dummy,'archetype14':dummy,'archetype15':dummy,'archetype16':dummy,'archetype17':dummy,'archetype18':dummy,'archetype19':dummy,'archetype20':dummy,'archetype21':dummy,'archetype22':dummy,'archetype23':dummy,'archetype24':dummy,'archetype25':dummy,'archetype26':dummy,'archetype27':dummy,'archetype28':dummy,'archetype29':dummy,'archetype30':dummy,'archetype31':dummy,'archetype32':dummy,'archetype33':dummy,'archetype34':dummy,'archetype35':dummy,'archetype36':dummy,'archetype37':dummy,'archetype38':dummy,'archetype39':dummy,'archetype40':dummy,'archetype41':dummy,'archetype42':dummy,'archetype43':dummy,'archetype44':dummy,'archetype45':dummy,'archetype46':dummy,'archetype47':dummy,'archetype48':dummy,'archetype49':dummy,'archetype50':dummy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the raw without Box Office value\n",
    "df_moviearchetypes = df_moviearchetypes.dropna(subset=['MovieBoxOfficeRevenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table will look like this\n",
    "df_moviearchetypes.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got the data.\n",
    "\n",
    "Our goal is to create a linear regression model with each archetype as a variable to predict the log of the box office revenue.\n",
    "\n",
    "We will use linear regression model and least square method to fit it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply log to boxoffice\n",
    "df_moviearchetypes['MovieBoxOfficeRevenue'] = df_moviearchetypes['MovieBoxOfficeRevenue'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of archetypes\n",
    "n = 50\n",
    "\n",
    "model_str = \"MovieBoxOfficeRevenue ~ \"\n",
    "for i in range(1,n+1):\n",
    "    model_str += \"C(archetype\" + str(i) + \")+\"\n",
    "\n",
    "model_str_without_interaction = model_str.strip(\"+\")\n",
    "\n",
    "for i in range(1,n):\n",
    "    for j in range(i+1, n+1):\n",
    "        model_str += \"C(archetype\" + str(i) + \"):C(archetype\" + str(j) + \")+\"\n",
    "\n",
    "model_str = model_str.strip(\"+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the model\n",
    "mod = smf.ols(formula = model_str_without_interaction, data = df_moviearchetypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model (adding a random seed ensuring consistency)\n",
    "np.random.seed(2)\n",
    "res = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary().tables[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared is very low.\n",
    "The model is not accurate.\n",
    "But we can still select which of those archetypes might have an effect on box office revenue by considering the p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will add interaction terms and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare the model\n",
    "mod = smf.ols(formula = model_str, data = df_moviearchetypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model \n",
    "res = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared is still low but  better than the last one. We can conclude that revenue prediction by archetype combination could be prospective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would be nice if someone could implement regression with three archetypes interaction term. In that case, we cannot use \"smf.ols\" anymore because of the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global sensitivity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to find out important archetypes without assuming linear model.  We will do variance-based analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have N variables of archetypes, box office revenue will be like $f(x_1,x_2\\dotsi x_N,\\epsilon)$.\\\n",
    "Note that each variable of $x_1,x_2\\dotsi x_N$ will take $0$ or $1$ and $\\epsilon$ denotes effects of other variables. And we assume $\\epsilon$ is independent of other variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f can be decomposed uniquely in a following way.\n",
    "\\begin{align}\n",
    "    f(\\boldsymbol{x}) &= f_\\phi + \\sum_{1<=i<=N}f_i(x_i)+ \\sum_{1<=i<j<=N}f_{i,j}(x_i,x_j)+\\sum_{1<=i<j<k<=N}f_{i,j,k}(x_i,x_j,x_k)+\\dotsi + f_\\epsilon(\\epsilon)\\nonumber\\\\\n",
    "    &= \\sum_{u\\subset\\{1,\\dotsi,N\\}}f_u(\\boldsymbol{x}_u) + f_\\epsilon(\\epsilon)\\\\\n",
    "    f_\\phi &= \\mathbb{E}[f] \\nonumber\\\\\n",
    "    f_j(x_j) &= \\mathbb{E}[f|x_j]-f_\\phi \\nonumber\\\\\n",
    "    f_{i,j}(x_i,x_j) &= \\mathbb{E}[f|x_i,x_j]-f_i(x_i)-f_j(x_j) - f_\\phi \\nonumber \\\\\n",
    "    &\\vdots \\nonumber\n",
    "\\end{align}\n",
    "This have good properties.\n",
    "+ $\\mathbb{E}[f_u] = 0$ when $u\\subset \\{1,\\dotsi,N\\}$ and $u\\neq \\phi$\n",
    "+ $\\mathbb{C}[f_u,f_v]=0$ when $u,v\\subset \\{1,\\dotsi,N\\}$ and $u\\neq v$\n",
    "\n",
    "Using these properties we can prove the following fact.\n",
    "\\begin{equation}\n",
    "    \\mathbb{V}[f] = \\sum_{u\\subset\\{1,\\dotsi,N\\}}\\mathbb{V}[f_u] + \\mathbb{V}[f_\\epsilon]\\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see ,for example, $\\mathbb{V}[f_1]$ as an effect of archetype 1 itself and $\\mathbb{V}[f_{1,2}]$ as an  effect of archetype 1and archetype 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we estimate$ \\mathbb{V}[f_u]$.\n",
    "The basic ideas is that if archetype sets $u\\subset\\{1,\\dotsi,N\\}$ is important, then the variance will be small when you fix $\\boldsymbol{x}_u$.\\\n",
    "We will resample from data and do Monte-Carlo estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $  \\boldsymbol{x} = (\\boldsymbol{x}_{u}, \\boldsymbol{x}_{-u})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\mathbb{V}[f_u]} = \\frac{1}{N}\\sum_{n=1}^N f(\\boldsymbol{x}_{-u}^{(n,1)},\\boldsymbol{x}_{u}^{(n)})f(\\boldsymbol{x}_{-u}^{(n,2)},\\boldsymbol{x}_{u}^{(n)}) - \\hat{\\mu}^{(1)}  \\hat{\\mu}^{(2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "where $(\\boldsymbol{x}_{u}^{(n)})_{n\\in[1:N]}$\n",
    "is an i.i.d. sample with the distribution of $\\boldsymbol{x}_{u}$\n",
    "and where\n",
    "$\\boldsymbol{x}_{-u}^{(n,1)}$and $\\boldsymbol{x}_{-u}^{(n,2)}$\n",
    "conditionally to $\\boldsymbol{x}_{u}$ are independent with the distribution\n",
    "of $\\boldsymbol{x}_{-u}$ conditionally to $\\boldsymbol{x}_{u} = \\boldsymbol{x}_{u}^{(n)}$\\\\\n",
    "$\\hat{\\mu}$ is an estimation of $\\mathbb{E}[f]$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "musquare = df_moviearchetypes.sample(N,replace=True).mean()[\"MovieBoxOfficeRevenue\"] *  df_moviearchetypes.sample(N,replace=True).mean()[\"MovieBoxOfficeRevenue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First_Order_Variance =np.zeros(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,51):\n",
    "    variance = 0\n",
    "    for j in range (N):\n",
    "        sample_u1 = df_moviearchetypes.sample(1)\n",
    "        sample_u2 =  df_moviearchetypes[df_moviearchetypes[\"archetype\"+str(i)] == int(sample_u1[\"archetype\"+str(i)])].sample(1)\n",
    "        variance += float(sample_u1[\"MovieBoxOfficeRevenue\"])* float(sample_u2[\"MovieBoxOfficeRevenue\"])\n",
    "    variance = variance / N-musquare\n",
    "    print(\"variance of archetype\"+str(i)+ \":\",variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,50):\n",
    "    for j in range(i+1,51):\n",
    "        variance = 0\n",
    "        for k in range (N):\n",
    "            sample_u1 = df_moviearchetypes.sample(1)\n",
    "            sample_u2 =  df_moviearchetypes[ (df_moviearchetypes[\"archetype\"+str(i)] == int(sample_u1[\"archetype\"+str(i)]) ) & (df_moviearchetypes[\"archetype\"+str(j)] == int(sample_u1[\"archetype\"+str(j)]) )].sample(1)\n",
    "            variance += float(sample_u1[\"MovieBoxOfficeRevenue\"])* float(sample_u2[\"MovieBoxOfficeRevenue\"])\n",
    "        variance = variance / N - musquare\n",
    "        print(\"variance of archetype\"+str(i)+\";\"+str(j)+ \":\",variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can tell if $\\hat{\\mathbb{V}[f_u]}$ is big, archetype sets $u$ might have significant effect on revenue wheter positively or negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
