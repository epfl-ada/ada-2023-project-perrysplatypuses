{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.character_attributes_extraction import character_names_from_text, character_attributes_from_text, character_active_verbs_from_text, character_patient_verbs_from_text\n",
    "from utils.clustering import get_lda_clusters, sort_meaningful\n",
    "from utils.clustering_evaluation import get_characters_with_tv_trop_info, group_labels_by_clusters, variation_of_information, unsupervised_evaluation\n",
    "from utils.imdb_ratings import movies_with_imdb_rating\n",
    "from utils.cluster_interpretation import plot_topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archetypes Revealing\n",
    "\n",
    "The basis of our data story is clustering: we grouped movie characters into 50 archetypes using the Latent Dirichlet Allocation method.\n",
    "\n",
    "For detailed explanations with examples and illustrations please refer\n",
    "- to `extract_character_attributes.ipynb` or extracting characters and their attributes from the plot texts\n",
    "\n",
    "- to `clustering.ipynb` for the clustering method: there you can find the methods comparison and the pipeline for characters clustering.\n",
    "\n",
    "Here we will show only the main logic\n",
    "\n",
    "## Attribute extraction\n",
    "\n",
    "For the attribute extraction we will use [Spacy](https://spacy.io/usage/linguistic-features) - library that allows to do many nlp tasks easily.\n",
    "We will primarily employ named entity recognition and dependency parsing. Additionally, for embeddings, we will utilize the built-in word2vec model.\n",
    "\n",
    "For every plot we find characters and their dependencies in 3 groups: attributes, active verbs and patient verbs and save it to csv file.\n",
    "As this procedure takes more than 5h to run, we just refer to `utils.character_attributes_extraction.py` and load attributes extracted by attributes_extraction function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_attributes =  pd.read_csv(\n",
    "    'data/character_attributes.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"adj\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), # need this to read list columns from csv\n",
    "        \"active\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "        \"patient\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "        }\n",
    "    )\n",
    "# keep only characters with >= 3 attributes\n",
    "characters_attributes = sort_meaningful(characters_attributes, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "First, we extract tv tropes as ground truth clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_to_check, tv_tropes = get_characters_with_tv_trop_info(characters_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use word2vec embeddings to do Latent Dirichlet Allocation with different numbers of topics and AgglomerativeClustering with different numbers of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative_clusters_n = [25, 50, 100]\n",
    "n_components = [25, 50, 100]\n",
    "\n",
    "configs = {}\n",
    "config_base = {'characters': characters_to_check, 'min_freq': 5, 'max_freq':0.9}\n",
    "\n",
    "for alg_n in agglomerative_clusters_n:\n",
    "    for n in n_components:\n",
    "        config = config_base.copy()\n",
    "        config['clustering_algo'] = AgglomerativeClustering(n_clusters=alg_n, metric='cosine', linkage='complete')\n",
    "        config['n_components'] = n\n",
    "        configs[f'{alg_n} topics, {n} archetypes'] = config\n",
    "\n",
    "results_lda = {}\n",
    "for k, config in configs.items():\n",
    "    clusters = get_lda_clusters(**config)\n",
    "    results_lda[k] = variation_of_information(group_labels_by_clusters(clusters), tv_tropes)\n",
    "    print(k, f'VI = {results_lda[k]}')\n",
    "\n",
    "clear_output(wait=True)\n",
    "results_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the results are even better (K=100, P=100, 5.42 in the paper and 4.9 here) than the results from the [paper](http://www.cs.cmu.edu/~dbamman/pubs/pdf/bamman+oconnor+smith.acl13.pdf). That could indicate that using word2vec embeddings and Agglomerative clustering of the words to topics might be better suited for dividing the words into topics for the purpose of personas extraction.\n",
    "\n",
    "We also experimented with clustering based on Bert embeddings: (refer to `utils/archive/transformer_embeddings.ipynb` for the pipeline):\n",
    "- obtain the character's embedding from the pre-trained BERT model\n",
    "- perform clustering (Agglomerative or KMeans) using these embeddings.\n",
    "\n",
    "So the LDA method first extracts linguistic features and then finds word2vec embeddings for them as soon as BERT-based clustering directly obtains the character’s embedding. Thus, LDA is a more explainable method because we know the attributes of every cluster.\n",
    "\n",
    "As the BERT-based method didn’t significantly outperform the LDA-based one, we chose LDA for our analysis due to its speed advantage and explainability.\n",
    "\n",
    "Then we experiment with different numbers of clusters (from 10 till 100 with step 5) to find the golden mean.\n",
    "Evaluation is done by Within-Cluster Sum of Squares (more - better) and Silhouette score (less - better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1000 of charactres for faster clustering\n",
    "characters_for_eval = characters_attributes.sample(1000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algo = AgglomerativeClustering(n_clusters=200, metric='cosine', linkage='complete')\n",
    "\n",
    "k_values = np.arange(10, 101, 5)\n",
    "wsss = []\n",
    "silhouettes = []\n",
    "min_wss_idx = 0\n",
    "\n",
    "for i in tqdm(range(len(k_values))):\n",
    "    y, X = get_lda_clusters(characters_for_eval, 5, 0.9, clustering_algo, k_values[i], return_topic_counts=True)\n",
    "    wss, silhouette = unsupervised_evaluation(X, y)\n",
    "    wsss.append(wss)\n",
    "    silhouettes.append(silhouette)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "x = k_values\n",
    "y1 = wsss\n",
    "y2 = silhouettes\n",
    "\n",
    "ax1.plot(x, y1, label='Within-Cluster Sum of Squares')\n",
    "ax1.set_xlabel('Number of clusters')\n",
    "ax1.set_ylabel('WSS')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, y2, label='Sillhouette score', linestyle='dashed')\n",
    "ax2.set_ylabel('Sillhouette')\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='upper right')\n",
    "plt.axvspan(50, 50.1, alpha=0.5,  color='darkblue')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that the 50 clusters provide relatively good combination of the silhouette and WSS scores and is still manageable to interpret. \n",
    "\n",
    "So we fix clustering, save the results and LDA topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algo = AgglomerativeClustering(n_clusters=200, metric='cosine', linkage='complete')\n",
    "\n",
    "optimal_k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# We want to save components of lda and word clusters (topic_dict) as well as clustering\n",
    "\n",
    "vocab, vocab_vectors = get_vocab(characters, 5, 0.9)\n",
    "topic_dict = word_topics_clustering(vocab, vocab_vectors, clustering_algo)\n",
    "counts = topic_count(characters, topic_dict)\n",
    "lda = LatentDirichletAllocation(\n",
    "        n_components=optimal_k, random_state=0\n",
    ").fit(counts)\n",
    "\n",
    "characters['cluster'] = lda.transform(counts).argmax(axis=1)\n",
    "characters.to_csv('data/character_clusters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "topic_to_words = {}\n",
    "\n",
    "for k, v in topic_dict.items():\n",
    "    v = str(v)\n",
    "    if v in topic_to_words:\n",
    "        topic_to_words[v].append(k)\n",
    "    else:\n",
    "        topic_to_words[v] = [k]\n",
    "        \n",
    "json.dump( topic_to_words, open( \"data/words_by_topic.json\", 'w' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "with open('data/lda_components.npy', 'wb') as f:\n",
    "    np.save(f, lda.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Now you understand how we implemented clustering so let's proceed to insights we got from these archetypes.\n",
    "\n",
    "We start with data loading:\n",
    "- Characters dataset was obtained by clustering\n",
    "- Movies, actors, plots were provided to us\n",
    "- Consumer Price Index (CPI) is taken from [U.S. Bureau of Labor Statistics CPI](https://www.bls.gov/cpi/data.htm) \n",
    "- Movie ratings dataset is taken from [IMDB Movies](https://developer.imdb.com/non-commercial-datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters =  pd.read_csv(\n",
    "    'data/character_clusters.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"adj\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), # need this to read list columns from csv\n",
    "        \"active\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "        \"patient\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "        }\n",
    "    )\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    'data/MovieSummaries/movie.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'title', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']\n",
    ")\n",
    "\n",
    "actors = pd.read_csv(\n",
    "    'data/MovieSummaries/character.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'release_date', 'character', 'date_of_birth', 'sex', 'height', '.','actor','age','character_map','..','...','....']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inflation data\n",
    "cpi_data = pd.read_csv('data/cpi_data.csv', )\n",
    "cpi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe combining actors data, character's name, movie id and clustering results (attributes and resulted cluster)\n",
    "def same_name(names1, names2):\n",
    "    names1 = names1.values\n",
    "    names2 = names2.values\n",
    "    flag = []\n",
    "    for i in range(len(names1)):\n",
    "        flag.append(names1[i] in names2[i])\n",
    "    return flag\n",
    "\n",
    "\n",
    "actors_and_characters = characters.merge(actors, how='left', left_on='wiki_id', right_on='wiki_id').dropna(subset=['character_y'])\n",
    "\n",
    "actors_and_characters = actors_and_characters[same_name(actors_and_characters['character_x'], actors_and_characters['character_y'])]\n",
    "actors_and_characters['character'] = actors_and_characters['character_x']\n",
    "actors_and_characters = actors_and_characters.drop(columns=['character_x', 'character_y'])\n",
    "actors_and_characters = actors_and_characters[['character', 'actor', 'cluster', 'wiki_id', 'release_date', 'date_of_birth', 'sex', 'height', 'age', 'adj', 'active', 'patient']]\n",
    "actors_and_characters.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add revenue discount and format movie dataframe\n",
    "\n",
    "def discount_revenue(year, revenue):\n",
    "    if year in cpi_data['year'].values:\n",
    "        cpi = cpi_data[cpi_data['year'] == year]['cpi'].values[0]\n",
    "    else:\n",
    "        cpi = 100\n",
    "    return (revenue /  cpi)*100\n",
    "\n",
    "\n",
    "map_dict_to_list = lambda x: [value for key, value in eval(x).items()]\n",
    "release_year = lambda x: pd.to_numeric(x.str.replace(r'-\\d{2}-\\d{2}$', '', regex=True).str.replace(r'-\\d{2}$', '', regex=True))\n",
    "\n",
    "movies['languages'] = movies['languages'].apply(map_dict_to_list)\n",
    "movies['countries'] = movies['countries'].apply(map_dict_to_list)\n",
    "movies['genres'] = movies['genres'].apply(map_dict_to_list)\n",
    "\n",
    "movies[\"release_year\"] = release_year(movies['release_date'])\n",
    "movies[\"release_year\"] = movies['release_year'].apply(lambda x: x if x > 1800 else x + 1000)\n",
    "\n",
    "movies['discounted_revenue'] = movies.apply(lambda x: discount_revenue(x.release_year, x.revenue), axis=1)\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# We use saved version of IMDB dataset\n",
    "movies_with_rating = movies_with_imdb_rating(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_with_rating = pd.read_csv(\n",
    "    'data/movies_with_rating.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"languages\": map_dict_to_list,\n",
    "        \"countries\": map_dict_to_list,\n",
    "        \"genres\": map_dict_to_list\n",
    "        }\n",
    "    )\n",
    "    \n",
    "movies_with_rating['release_year'] = release_year(movies_with_rating['release_date'])\n",
    "movies_with_rating['discounted_revenue'] = movies_with_rating.apply(lambda x: discount_revenue(x.release_year, x.revenue), axis=1)\n",
    "\n",
    "movies_with_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = pd.read_csv(\n",
    "    'data/MovieSummaries/plot_summaries.txt', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'plot']\n",
    ")\n",
    "movies_and_plots = movies.merge(plots, how='right', left_on='wiki_id', right_on='wiki_id')\n",
    "num_plot = len(pd.unique(movies_and_plots['wiki_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_and_movies = characters.merge(movies, left_on='wiki_id', right_on='wiki_id')\n",
    "num_char = len(pd.unique(characters_and_movies['wiki_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of movies: {len(movies)}\")\n",
    "print(f\"Number of movies with revenue: {movies['revenue'].notna().sum()}\")\n",
    "print(f\"Number of movies with rating: {len(movies_with_rating)}\")\n",
    "print(f\"Number of movies with rating and revenue: {movies_with_rating['revenue'].notna().sum()}\")\n",
    "print(f\"Number of movies with plot: {num_plot}\")\n",
    "print(f\"Number of movies, where we find archetypes: {num_char}\")\n",
    "print(f\"Number of actors with the characters who have an archetype: {len(actors_and_characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Obtain the data\n",
    "categories_movie_plot = [\"Total Movies\", \"Movies with Plot\", \"Movies with Archetypes\"]\n",
    "\n",
    "movie_values = [len(movies),\n",
    "          num_plot,\n",
    "          num_char]\n",
    "\n",
    "# Create a bar chart\n",
    "fig = go.Figure(data=[go.Bar(x=categories_movie_plot, y=movie_values)])\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    title=\"Statistics of Movies with Plots and Archetypes\",\n",
    "    xaxis_title=\"Categories\",\n",
    "    yaxis_title=\"Number of Movies\",\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that nearly half of the movies have plot data, and archetypes have been extracted from only one-third of them.\n",
    "\n",
    "To explain these results, let's take a look at plots length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sentence):\n",
    "    words = sentence.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_with_characters = characters_and_movies['wiki_id'].values\n",
    "movies_with_arch = movies_and_plots.loc[movies_and_plots['wiki_id'].isin(id_with_characters)]\n",
    "plot_length_with_arch = movies_with_arch['plot'].apply(count_words) \n",
    "plot_length = movies_and_plots['plot'].apply(count_words) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 50 log-scale bins\n",
    "_, bins_plot = np.histogram(plot_length, bins=50)\n",
    "_, bins_arch = np.histogram(plot_length_with_arch, bins=50)\n",
    "logbins_plot = np.logspace(np.log10(bins_plot[0]),np.log10(bins_plot[-1]),len(bins_plot))\n",
    "logbins_arch = np.logspace(np.log10(bins_arch[0]),np.log10(bins_arch[-1]),len(bins_arch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Subplot 1\n",
    "axes[0].hist(plot_length, bins=logbins_plot, alpha=0.5)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Log of the plot length in terms of words')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_ylim(0,2100)\n",
    "axes[0].set_title(\"Distribution of the plot length in terms of words\")\n",
    "axes[0].axvspan(100, 101, alpha=0.3, label='100 words', color='darkblue')\n",
    "\n",
    "# Subplot 2\n",
    "axes[1].hist(plot_length_with_arch, bins=logbins_arch, alpha=0.5)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Log of the plot length in terms of words')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_ylim(0,2100)\n",
    "axes[1].set_title(\"Distribution of the plot length in terms of words for discovered archetypes\")\n",
    "axes[1].axvspan(100, 101, alpha=0.3, label='100 words', color='darkblue')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see clear pattern: in the movies with short plot we obtain archetypes with the lower probability.\n",
    "\n",
    "If we talk about actors, only small fraction of them played a character with defined archetype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T14:26:21.282081Z",
     "iopub.status.busy": "2023-12-20T14:26:21.281727Z",
     "iopub.status.idle": "2023-12-20T14:26:21.295763Z",
     "shell.execute_reply": "2023-12-20T14:26:21.295259Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of actors: {len(actors)}\")\n",
    "print(f\"Number of actors with the characters who have an archetype: {len(actors_and_characters)}\")\n",
    "print(f\"Number of actors with the characters who have an archetype in the movies with revenue and rating: {len(actors_and_characters[actors_and_characters['wiki_id'].isin(movies_with_rating[movies_with_rating['revenue'].notna()]['wiki_id'])])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the countries of production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coutries_distr = movies.explode('countries').groupby('countries').size()\n",
    "coutries_distr_with_rating = movies_with_rating.explode('countries').groupby('countries').size()\n",
    "coutries_distr_with_rating_and_revenue = movies_with_rating[movies_with_rating['revenue'].notna()].explode('countries').groupby('countries').size()\n",
    "\n",
    "coutries = list(set(\n",
    "    coutries_distr.sort_values(ascending=False)[:20].index.to_list() \n",
    "    + coutries_distr_with_rating.sort_values(ascending=False)[:20].index.to_list() \n",
    "    + coutries_distr_with_rating_and_revenue.sort_values(ascending=False)[:20].index.to_list()))\n",
    "\n",
    "coutries_distr = coutries_distr.loc[coutries].sort_values(ascending=True)\n",
    "coutries = coutries_distr.index.to_list() \n",
    "coutries_distr_with_rating = coutries_distr_with_rating.loc[coutries]\n",
    "coutries_distr_with_rating_and_revenue = coutries_distr_with_rating_and_revenue.loc[coutries]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title('Top of movie production countries')\n",
    "\n",
    "plt.barh(coutries_distr.index, coutries_distr.values, label='all movies')\n",
    "plt.barh(coutries_distr_with_rating.index, coutries_distr_with_rating.values, label='movies with rating')\n",
    "plt.barh(coutries_distr_with_rating_and_revenue.index, coutries_distr_with_rating_and_revenue.values, label='movies with rating and revenue')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that most of the movies in the dataset are made in the US, moreover, we have much less data for movies with revenue and this data is't distributed prportionally to the overall number of movies produced in the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the historical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.groupby('release_year').size().plot(figsize=(15, 5), title='Number of released movies', label='number of released movies')\n",
    "plt.xticks(np.arange(1890, 2021, 7))\n",
    "\n",
    "plt.axvspan(1914, 1918, alpha=0.3, label='World War I')\n",
    "plt.axvspan(1929, 1939, alpha=0.3, label='Great Depression', color='green')\n",
    "plt.axvspan(1939, 1945, alpha=0.3, label='World War II')\n",
    "plt.axvspan(1961.2, 1961.3, alpha=0.3, label='First space flight', color='purple')\n",
    "plt.axvspan(2007, 2008, alpha=0.3, label='Global Financial Crisis', color='green')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have much data before 1910-s and after 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters interpretation\n",
    "To interpret clusters, we can use the function `plot_topic_distribution` to see the topics with the largest probabilities to be in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historycal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_count = characters_and_movies.groupby('release_year').size().reset_index(name='movie_count')\n",
    "movies_count = movies_count[movies_count['movie_count'] >= 15]\n",
    "movies_count.plot(x='release_year', y='movie_count')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: We decide to analyze trends where there is a stable abundance of data, and remove movies before 1932 and tha last two years (2013-2014). For further analysis we are selecting important clusters (by relative popularity or changes in popularity) but this selection is skewed by the years where there is little data since that gives a very high proportion for every cluster. So the early clusters will appear very significant despite that not being the case (if e.g. there are only a handful of movies, the archetype distribution is not very interesting). Therefore the filtered subset is used, not only for plot, but also for cluster ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_counts = characters_and_movies[characters_and_movies['release_year'].isin(movies_count['release_year'])].groupby(['release_year', 'cluster']).size().reset_index(name='character_count')\n",
    "archetype_counts = archetype_counts.pivot(index='release_year', columns='cluster', values='character_count').fillna(0)\n",
    "archetype_counts.plot(legend=False, linewidth = .6)\n",
    "plt.xlabel(\"Release year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_archetype_counts = (archetype_counts)/(archetype_counts.values.sum(1).reshape(-1, 1))\n",
    "normalized_archetype_counts.plot(legend=False, linewidth = .6)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Release year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to see any patterns here, so we will analyze top archetypes only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top archetypes\n",
    "\n",
    "- By the highest sum of normalized frequency (popularity)\n",
    "- By the biggest range in normalized frequency (changes in popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_archetype_counts = normalized_archetype_counts.loc[(normalized_archetype_counts.index <= 2012) & (normalized_archetype_counts.index >= 1932)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of normalized frequency\n",
    "\n",
    "top_clusters = normalized_archetype_counts.sum(0).sort_values(ascending=False)[:5].index.values\n",
    "top_clusters_archetype_counts = normalized_archetype_counts[top_clusters]\n",
    "\n",
    "# biggest range in normalized frequency\n",
    "\n",
    "top_diff_clusters = normalized_archetype_counts.apply(np.ptp).sort_values(ascending=False)[:5].index.values\n",
    "top_clusters_archetype_diff = normalized_archetype_counts[top_diff_clusters]\n",
    "top_clusters_archetype_diff\n",
    "\n",
    "print(\"top_clusters_archetype_counts:\", list(top_clusters_archetype_counts.columns), \"\\n\" +\n",
    "      \"top_clusters_archetype_diff:\", list(top_clusters_archetype_diff.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top clusters for both selections are the same (but in a different order), which is convenient for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clusters_archetype_counts.plot(figsize=(12, 6), linewidth = .7)\n",
    "#plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title('Normalized character counts for top clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "n = 10 # sliding average window size\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over clusters and plot a line for each\n",
    "for cluster in top_clusters:\n",
    "    x = top_clusters_archetype_counts[cluster]\n",
    "    x_avg = np.convolve(x, np.ones(n)/n, mode='valid')\n",
    "    y = top_clusters_archetype_counts.index\n",
    "    y_1 = y[round(n/2):-(n-round(n/2))+1]\n",
    "    plt.plot(y_1, x_avg, label=f'Cluster {cluster}', marker='', linewidth=0.7)\n",
    "\n",
    "#plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title(f'Normalized character counts for top clusters. Sliding average (n={n})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_info(n):\n",
    "    print('Cluster: ', n)\n",
    "    top = characters_and_movies[(characters_and_movies['cluster'] == n) & (characters_and_movies['revenue'] > 5e8)]\n",
    "    top = top.sort_values(by='revenue', ascending=False).head(5)\n",
    "    print(top[['title', 'character']])\n",
    "    plot_topic_distribution(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in top_clusters[:5]:\n",
    "    print_cluster_info(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the three most popular archetypes are all archetypes of different kinds of protagonists and their close allies, and the archetypes that changed in popularity are mostly side caracters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cultural preference\n",
    "\n",
    "We are interested in the cultural preferences at more modern times, so we will look only at the data from the 21 century. We also will use only the first country in the list of production countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_movies_countries = characters_and_movies[characters_and_movies['release_year'] > 2000]\n",
    "char_movies_countries['countries'] = char_movies_countries['countries'].apply(lambda x: x[0] if x else 'Unknown')\n",
    "\n",
    "char_movies_countries = char_movies_countries.groupby(['countries', 'cluster']).size().reset_index(name='character_count')\n",
    "\n",
    "char_movies_countries = char_movies_countries[['countries', 'cluster', 'character_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top production countries\n",
    "char_movies_countries.groupby('countries')['character_count'].sum().sort_values(ascending=False)[:50].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_by_country = char_movies_countries.pivot(index='countries', columns='cluster', values='character_count').fillna(0)\n",
    "archetype_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_by_continent = archetype_by_country.copy()\n",
    "\n",
    "# Group by continent and sum the values for each cluster\n",
    "archetype_by_continent['continent'] = archetype_by_continent.index.map(country_to_continent)\n",
    "archetype_by_continent = archetype_by_continent.groupby('continent').sum()\n",
    "\n",
    "# scaling frequencies to percentage for every country\n",
    "df_scaled = archetype_by_continent.div(archetype_by_continent.sum(axis=1), axis=0)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "sns.heatmap(df_scaled, cmap='YlGnBu', annot=False, fmt='g', cbar_kws={'label': 'Frequency [%]'})\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a lot of films with unknown country of production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_countries = char_movies_countries.groupby('countries')['character_count'].sum().sort_values(ascending=False)[:11].index\n",
    "top_countries = top_countries.drop('Unknown')\n",
    "top_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_movies_countries = char_movies_countries[char_movies_countries['countries'].isin(top_countries)]\n",
    "archetype_by_country = char_movies_countries.pivot(index='countries', columns='cluster', values='character_count').fillna(0)\n",
    "archetype_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_movies_countries.groupby('countries')['character_count'].sum().plot.barh(x='countries')\n",
    "\n",
    "plt.ylabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have much more data on the american films, so we will normalize the data. After that let's look at the distribution of the global top 5 archetypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_archetype_by_country = (archetype_by_country)/(archetype_by_country.values.sum(1).reshape(-1, 1))\n",
    "\n",
    "normalized_archetype_by_country[top_clusters[:5]].plot.barh(figsize=(7, 10), title='Distribution of the top 5 clusters in top 10 countries')\n",
    "\n",
    "plt.ylabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to notice the difference between this countries. We can notice the difference in the types of the most popular protagonists for example in India and Hong Kong compared to United Kingdom and Spain. In the first group, the most popular protagonists are those who act and achieve something, while in the second group the most popular protagonists are communicating more and travel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can't say, that the distribution of the archetypes is different in different countries. But was it the case in 20th century?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_movies_countries = characters_and_movies[characters_and_movies['release_year'] < 2001]\n",
    "char_movies_countries['countries'] = char_movies_countries['countries'].apply(lambda x: x[0] if x else 'Unknown')\n",
    "\n",
    "char_movies_countries = char_movies_countries.groupby(['countries', 'cluster']).size().reset_index(name='character_count')\n",
    "\n",
    "char_movies_countries = char_movies_countries[['countries', 'cluster', 'character_count']]\n",
    "\n",
    "char_movies_countries = char_movies_countries[char_movies_countries['countries'].isin(top_countries)]\n",
    "archetype_by_country = char_movies_countries.pivot(index='countries', columns='cluster', values='character_count').fillna(0)\n",
    "\n",
    "normalized_archetype_by_country = (archetype_by_country)/(archetype_by_country.values.sum(1).reshape(-1, 1))\n",
    "\n",
    "normalized_archetype_by_country[top_clusters[:5]].plot.barh(figsize=(7, 10), title='Distribution of the top 5 clusters in top 10 countries in the 20th century')\n",
    "\n",
    "plt.ylabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the shift that hapend from the 20th to 21st century from more achieving to communicating protagonists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie success based on the archetypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model for revenue prediction\n",
    "\n",
    "To determine importance of the archetypes for the movie success, we can build the linear model and tell what are the most important archetypes based on the coefficient and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_and_revenue = characters_and_movies[characters_and_movies['discounted_revenue'].notna()][['wiki_id', 'discounted_revenue']]\n",
    "cluster_and_revenue['log_revenue'] = np.log(cluster_and_revenue['discounted_revenue'])\n",
    "cluster_and_revenue = cluster_and_revenue[['wiki_id', 'log_revenue']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "revenues = cluster_and_revenue['log_revenue'].values\n",
    "wiki_ids = cluster_and_revenue['wiki_id'].values\n",
    "\n",
    "plt.hist(revenues, bins=50)\n",
    "plt.title('Log revenue histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Obtain the data\n",
    "categories_movie_plot = [\"Movies with Revenue\", \"Movies with Rating\", \"Movies with Rating and Revenue\"]\n",
    "\n",
    "movie_values = [movies['revenue'].notna().sum(),\n",
    "          len(movies_with_rating),\n",
    "          movies_with_rating['revenue'].notna().sum()]\n",
    "\n",
    "# Create a bar chart\n",
    "fig = go.Figure(data=[go.Bar(x=categories_movie_plot, y=movie_values)])\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    title=\"Statistics of Movies with Revenue and Rating\",\n",
    "    xaxis_title=\"Categories\",\n",
    "    yaxis_title=\"Number of Movies\",\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T14:26:29.590194Z",
     "iopub.status.busy": "2023-12-20T14:26:29.589988Z",
     "iopub.status.idle": "2023-12-20T14:26:32.128043Z",
     "shell.execute_reply": "2023-12-20T14:26:32.127317Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters = np.zeros((len(cluster_and_revenue), 50)) #initializing the clusters\n",
    "for i in range(len(wiki_ids)):\n",
    "    wiki_id = wiki_ids[i]\n",
    "    for c in characters_and_movies[characters_and_movies['wiki_id'] == wiki_id]['cluster'].values:\n",
    "        clusters[i][c] = 1\n",
    "        \n",
    "\n",
    "cluster_revenue_data = pd.DataFrame(clusters, columns=[f'archetype_{i}' for i in np.arange(50)])\n",
    "cluster_revenue_data['log_revenue'] = cluster_and_revenue['log_revenue']\n",
    "cluster_revenue_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "\n",
    "model_str = \"log_revenue ~ \"\n",
    "for i in range(n):\n",
    "    model_str += \"C(archetype_\" + str(i) + \")+\"\n",
    "\n",
    "model_str_without_interaction = model_str.strip(\"+\")\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        model_str += \"C(archetype_\" + str(i) + \"):C(archetype_\" + str(j) + \")+\"\n",
    "\n",
    "model_str = model_str.strip(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula = model_str_without_interaction, data = cluster_revenue_data)\n",
    "res_without_interaction = mod.fit()\n",
    "res_without_interaction.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula = model_str, data = cluster_revenue_data)\n",
    "res = mod.fit()\n",
    "res.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the significant improvement in R-squared metric, we can say that interactions between archetypes are important.\n",
    "\n",
    "Next, let's look at p-values and coefficents to find out important archetypes and interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_coefficients = res.params[res.pvalues <= 0.05].abs().sort_values(ascending=False)\n",
    "sorted_coefficients = res.params.loc[significant_coefficients.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_coefficients[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example,the combination of archetype_22 and archetype_44 could be a bad combination and archetype_7 and archetype_25 could be a bad combination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at anova results to determine the important archetypes and interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(res, robust='hc3').sort_values('PR(>F)')[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors success based on the archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rating_revenue = movies_with_rating[['wiki_id', 'discounted_revenue', 'averageRating']].dropna(subset=['discounted_revenue']).reset_index(drop=True)\n",
    "movies_rating_revenue['discounted_revenue'] = np.log(movies_rating_revenue['discounted_revenue'])\n",
    "movies_rating_revenue['norm_log_revenue'] = (movies_rating_revenue['discounted_revenue'] - np.min(movies_rating_revenue['discounted_revenue'])) * 10/ (np.max(movies_rating_revenue['discounted_revenue']) - np.min(movies_rating_revenue['discounted_revenue']))\n",
    "movies_rating_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rating_revenue[['norm_log_revenue', 'averageRating']].plot.hist(alpha=0.3, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearsonr(movies_rating_revenue['norm_log_revenue'].values, movies_rating_revenue['averageRating'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that there is some statistically significant correlation, even though it's not wery big. We will use the sum of normalized log revenue and rating of the film as the metric for success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rating_revenue['success'] = movies_rating_revenue['averageRating'] + movies_rating_revenue['norm_log_revenue']\n",
    "movies_rating_revenue['success'].plot.hist(alpha=0.3, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_and_characters['importance'] = actors_and_characters['adj'].apply(len) + actors_and_characters['active'].apply(len) + actors_and_characters['patient'].apply(len)\n",
    "actors_and_characters['importance'] = actors_and_characters['importance'] / actors_and_characters.groupby('wiki_id')['importance'].transform('sum')\n",
    "actors_and_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_and_characters_with_success = actors_and_characters.merge(movies_rating_revenue[['wiki_id', 'success']], left_on='wiki_id', right_on='wiki_id')\n",
    "actors_and_characters_with_success = actors_and_characters_with_success[['actor', 'cluster', 'date_of_birth', 'sex', 'height', 'age', 'importance', 'success']]\n",
    "actors_and_characters_with_success.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_and_characters_with_success['weighted_success'] = actors_and_characters_with_success['importance'] * actors_and_characters_with_success['success']\n",
    "top_actors = actors_and_characters_with_success.groupby('actor').agg({'weighted_success': ['sum', 'size'], 'cluster':list, 'sex':'last', 'date_of_birth':'last'}).sort_values(('weighted_success',  'sum'), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only look at the actors with at least 5 films\n",
    "top_actors.columns = ['sum_success', 'num_films', 'clusters', 'sex', 'date_of_birth']\n",
    "top_actors = top_actors.reset_index()\n",
    "top_actors = top_actors[top_actors['num_films'] > 4]\n",
    "top_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_actors['year_of_birth'] = top_actors['date_of_birth'].apply(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sum_success_plot = sns.scatterplot(data=top_actors[:50], x='actor', y='sum_success', hue='num_films')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archetype number and actor's success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_actors['cluster_number'] = top_actors['clusters'].apply(lambda x: len(set(x)))\n",
    "top_actors['clusters_to_films_ratio'] = top_actors['cluster_number']/top_actors['num_films']\n",
    "top_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(top_actors['clusters_to_films_ratio'], bins=20)\n",
    "print('Median of the cluster to film ratio: ', np.median(top_actors['clusters_to_films_ratio']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_actors['many_archetypes'] = top_actors['clusters_to_films_ratio'].apply(lambda x: int(x > 0.70))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform causal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_top_actors_data = top_actors[['sum_success', 'many_archetypes', 'year_of_birth', 'sex', 'num_films']].reset_index(drop=True)\n",
    "norm_top_actors_data['sex'] =  norm_top_actors_data['sex'].apply(lambda x: int(x=='F'))\n",
    "norm_top_actors_data['year_of_birth'] = norm_top_actors_data['year_of_birth'] - norm_top_actors_data['year_of_birth'].mean() / norm_top_actors_data['year_of_birth'].std()\n",
    "norm_top_actors_data['num_films'] = norm_top_actors_data['num_films'] - norm_top_actors_data['num_films'].mean() / norm_top_actors_data['num_films'].std()\n",
    "\n",
    "\n",
    "mod = smf.logit(formula='many_archetypes ~  year_of_birth + sex + num_films', data=norm_top_actors_data)\n",
    "res = mod.fit()\n",
    "\n",
    "# Extract the estimated propensity scores\n",
    "norm_top_actors_data['Propensity_score'] = res.predict()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)\n",
    "\n",
    "treatment_df = norm_top_actors_data[norm_top_actors_data['many_archetypes'] == 1]\n",
    "control_df = norm_top_actors_data[norm_top_actors_data['many_archetypes'] == 0]\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                    treatment_row['Propensity_score'])\n",
    "\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "matching = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "balanced_norm_top_actors_data = norm_top_actors_data.iloc[matched]\n",
    "balanced_norm_top_actors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = balanced_norm_top_actors_data.loc[balanced_norm_top_actors_data['many_archetypes'] == 1]\n",
    "control = balanced_norm_top_actors_data.loc[balanced_norm_top_actors_data['many_archetypes'] == 0]\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "ax = sns.histplot(treated['sum_success'], kde=True, stat='density', color='blue', label='many_archetypes');\n",
    "ax = sns.histplot(control['sum_success'], kde=True, stat='density', color='orange', label='not many_archetypes')\n",
    "ax.set(title='Success distribution comparison',xlabel='sum success', ylabel='density')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Sum success\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(treated['sum_success'],control['sum_success'], alternative='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can say that ators, who have played less archetypes are statistically significantly more successful that those, who played more various archetypes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
