{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.imdb_ratings import movies_with_imdb_rating\n",
    "from utils.cluster_interpretation import plot_topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What isn't included in this notebook\n",
    "\n",
    "This project required a lot of preprocessing, which is an interesting task, but is not related to the research questions. In this notebook we will focus on the research questions only.\n",
    "\n",
    "For extracting characters and their attributes from the plot texts, refer to `extract_character_attributes.ipynb`.\n",
    "\n",
    "For the clustering method please refer to `clustering.ipynb`, there you can find the methods comparison and the pipeline for characters clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters =  pd.read_csv(\n",
    "    'data/character_clusters.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"adj\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"), # need this to read list columns from csv\n",
    "        \"active\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \"),\n",
    "        \"patient\": lambda x: x.strip(\"[]\").replace(\"'\",\"\").split(\", \")\n",
    "        }\n",
    "    )\n",
    "\n",
    "movies = pd.read_csv(\n",
    "    'data/MovieSummaries/movie.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'title', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres']\n",
    ")\n",
    "\n",
    "actors = pd.read_csv(\n",
    "    'data/MovieSummaries/character.metadata.tsv', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'freebase_id', 'release_date', 'character', 'date_of_birth', 'sex', 'height', '.','actor','age','character_map','..','...','....']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_data = pd.read_csv('data/cpi_data.csv', )\n",
    "cpi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_name(names1, names2):\n",
    "    names1 = names1.values\n",
    "    names2 = names2.values\n",
    "    flag = []\n",
    "    for i in range(len(names1)):\n",
    "        flag.append(names1[i] in names2[i])\n",
    "    return flag\n",
    "\n",
    "\n",
    "actors_and_characters = characters.merge(actors, how='left', left_on='wiki_id', right_on='wiki_id').dropna(subset=['character_y'])\n",
    "\n",
    "actors_and_characters = actors_and_characters[same_name(actors_and_characters['character_x'], actors_and_characters['character_y'])]\n",
    "actors_and_characters['character'] = actors_and_characters['character_x']\n",
    "actors_and_characters = actors_and_characters.drop(columns=['character_x', 'character_y'])\n",
    "actors_and_characters = actors_and_characters[['character', 'actor', 'cluster', 'wiki_id', 'release_date', 'date_of_birth', 'sex', 'height', 'age', 'adj', 'active', 'patient']]\n",
    "actors_and_characters.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_revenue(year, revenue):\n",
    "    if year in cpi_data['year'].values:\n",
    "        cpi = cpi_data[cpi_data['year'] == year]['cpi'].values[0]\n",
    "    else:\n",
    "        cpi = 100\n",
    "    return (revenue /  cpi)*100\n",
    "\n",
    "\n",
    "map_dict_to_list = lambda x: [value for key, value in eval(x).items()]\n",
    "release_year = lambda x: pd.to_numeric(x.str.replace(r'-\\d{2}-\\d{2}$', '', regex=True).str.replace(r'-\\d{2}$', '', regex=True))\n",
    "\n",
    "movies['languages'] = movies['languages'].apply(map_dict_to_list)\n",
    "movies['countries'] = movies['countries'].apply(map_dict_to_list)\n",
    "movies['genres'] = movies['genres'].apply(map_dict_to_list)\n",
    "\n",
    "movies[\"release_year\"] = release_year(movies['release_date'])\n",
    "movies[\"release_year\"] = movies['release_year'].apply(lambda x: x if x > 1800 else x + 1000)\n",
    "\n",
    "movies['discounted_revenue'] = movies.apply(lambda x: discount_revenue(x.release_year, x.revenue), axis=1)\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Script takes time to run, so we will use saved version instead\n",
    "movies_with_rating = movies_with_imdb_rating(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_with_rating = pd.read_csv(\n",
    "    'data/movies_with_rating.csv', \n",
    "    index_col=0,\n",
    "    converters={\n",
    "        \"languages\": map_dict_to_list,\n",
    "        \"countries\": map_dict_to_list,\n",
    "        \"genres\": map_dict_to_list\n",
    "        }\n",
    "    )\n",
    "    \n",
    "movies_with_rating['release_year'] = release_year(movies_with_rating['release_date'])\n",
    "movies_with_rating['discounted_revenue'] = movies_with_rating.apply(lambda x: discount_revenue(x.release_year, x.revenue), axis=1)\n",
    "\n",
    "movies_with_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = pd.read_csv(\n",
    "    'data/MovieSummaries/plot_summaries.txt', \n",
    "    sep='\\t', \n",
    "    names=['wiki_id', 'plot']\n",
    ")\n",
    "movies_and_plots = movies.merge(plots, how='right', left_on='wiki_id', right_on='wiki_id')\n",
    "num_plot = len(pd.unique(movies_and_plots['wiki_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_and_movies = characters.merge(movies, left_on='wiki_id', right_on='wiki_id')\n",
    "num_char = len(pd.unique(characters_and_movies['wiki_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of movies: {len(movies)}\")\n",
    "print(f\"Number of movies with revenue: {movies['revenue'].notna().sum()}\")\n",
    "print(f\"Number of movies with rating: {len(movies_with_rating)}\")\n",
    "print(f\"Number of movies with rating and revenue: {movies_with_rating['revenue'].notna().sum()}\")\n",
    "print(f\"Number of movies with plot: {num_plot}\")\n",
    "print(f\"Number of movies, where we find archetypes: {num_char}\")\n",
    "print(f\"Number of actors with the characters who have an archetype: {len(actors_and_characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Obtain the data\n",
    "categories_movie_plot = [\"Total Movies\", \"Movies with Plot\", \"Movies with Archetypes\"]\n",
    "\n",
    "movie_values = [len(movies),\n",
    "          num_plot,\n",
    "          num_char]\n",
    "\n",
    "# Create a bar chart\n",
    "fig = go.Figure(data=[go.Bar(x=categories_movie_plot, y=movie_values)])\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    title=\"Statistics of Movies with Plots and Archetypes\",\n",
    "    xaxis_title=\"Categories\",\n",
    "    yaxis_title=\"Number of Movies\",\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T14:26:21.282081Z",
     "iopub.status.busy": "2023-12-20T14:26:21.281727Z",
     "iopub.status.idle": "2023-12-20T14:26:21.295763Z",
     "shell.execute_reply": "2023-12-20T14:26:21.295259Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of characters with archetypes: {len(characters)}\")\n",
    "print(f\"Number of actors: {len(actors)}\")\n",
    "print(f\"Number of actors with the characters who have an archetype: {len(actors_and_characters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of actors with the characters who have an archetype in the movies with revenue and rating: {len(actors_and_characters[actors_and_characters['wiki_id'].isin(movies_with_rating[movies_with_rating['revenue'].notna()]['wiki_id'])])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the countries of production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coutries_distr = movies.explode('countries').groupby('countries').size()\n",
    "coutries_distr_with_rating = movies_with_rating.explode('countries').groupby('countries').size()\n",
    "coutries_distr_with_rating_and_revenue = movies_with_rating[movies_with_rating['revenue'].notna()].explode('countries').groupby('countries').size()\n",
    "\n",
    "coutries = list(set(\n",
    "    coutries_distr.sort_values(ascending=False)[:20].index.to_list() \n",
    "    + coutries_distr_with_rating.sort_values(ascending=False)[:20].index.to_list() \n",
    "    + coutries_distr_with_rating_and_revenue.sort_values(ascending=False)[:20].index.to_list()))\n",
    "\n",
    "coutries_distr = coutries_distr.loc[coutries].sort_values(ascending=True)\n",
    "coutries = coutries_distr.index.to_list() \n",
    "coutries_distr_with_rating = coutries_distr_with_rating.loc[coutries]\n",
    "coutries_distr_with_rating_and_revenue = coutries_distr_with_rating_and_revenue.loc[coutries]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.title('Top of movie production countries')\n",
    "\n",
    "plt.barh(coutries_distr.index, coutries_distr.values, label='all movies')\n",
    "plt.barh(coutries_distr_with_rating.index, coutries_distr_with_rating.values, label='movies with rating')\n",
    "plt.barh(coutries_distr_with_rating_and_revenue.index, coutries_distr_with_rating_and_revenue.values, label='movies with rating and revenue')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that most of the movies in the dataset are made in the US, moreover, we have much less data for movies with revenue and this data is't distributed prportionally to the overall number of movies produced in the country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the historical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.groupby('release_year').size().plot(figsize=(15, 5), title='Number of released movies', label='number of released movies')\n",
    "plt.xticks(np.arange(1890, 2021, 7))\n",
    "\n",
    "plt.axvspan(1914, 1918, alpha=0.3, label='World War I')\n",
    "plt.axvspan(1929, 1939, alpha=0.3, label='Great Depression', color='green')\n",
    "plt.axvspan(1939, 1945, alpha=0.3, label='World War II')\n",
    "plt.axvspan(1961.2, 1961.3, alpha=0.3, label='First space flight', color='purple')\n",
    "plt.axvspan(2007, 2008, alpha=0.3, label='Global Financial Crisis', color='green')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have much data before 1910-s and after 2012."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters interpretation\n",
    "To interpret clusters, we can use the function `plot_topic_distribution` to see the topics with the largest probabilities to be in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic_distribution(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historycal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_count = characters_and_movies.groupby('release_year').size().reset_index(name='movie_count')\n",
    "movies_count = movies_count[movies_count['movie_count'] >= 15]\n",
    "movies_count.plot(x='release_year', y='movie_count')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: We decide to analyze trends where there is a stable abundance of data, and remove movies before 1932 and tha last two years (2013-2014). For further analysis we are selecting important clusters (by relative popularity or changes in popularity) but this selection is skewed by the years where there is little data since that gives a very high proportion for every cluster. So the early clusters will appear very significant despite that not being the case (if e.g. there are only a handful of movies, the archetype distribution is not very interesting). Therefore the filtered subset is used, not only for plot, but also for cluster ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archetype_counts = characters_and_movies[characters_and_movies['release_year'].isin(movies_count['release_year'])].groupby(['release_year', 'cluster']).size().reset_index(name='character_count')\n",
    "archetype_counts = archetype_counts.pivot(index='release_year', columns='cluster', values='character_count').fillna(0)\n",
    "archetype_counts.plot(legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_archetype_counts = (archetype_counts)/(archetype_counts.values.sum(1).reshape(-1, 1))\n",
    "normalized_archetype_counts.plot(legend=False)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top archetypes\n",
    "\n",
    "- By the highest sum of normalized frequency (popularity)\n",
    "- By the biggest range in normalized frequency (changes in popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of normalized frequency\n",
    "\n",
    "top_clusters = normalized_archetype_counts.sum(0).sort_values(ascending=False)[:10].index.values\n",
    "top_clusters_archetype_counts = normalized_archetype_counts[top_clusters]\n",
    "top_clusters_archetype_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clusters_archetype_counts.plot(figsize=(12, 6))\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title('Normalized character counts by cluster: subset 1')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "n = 10 # sliding average window size\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over clusters and plot a line for each\n",
    "for cluster in top_clusters:\n",
    "    x = top_clusters_archetype_counts[cluster]\n",
    "    x_avg = np.convolve(x, np.ones(n)/n, mode='valid')\n",
    "    y = top_clusters_archetype_counts.index\n",
    "    y_1 = y[round(n/2):-(n-round(n/2))+1]\n",
    "    plt.plot(y_1, x_avg, label=f'Cluster {cluster}', marker='', linewidth=0.7)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title(f'Normalized character counts by cluster: subset 1. Sliding average (n={n})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_info(n):\n",
    "    print('Cluster: ', n)\n",
    "    top = characters_and_movies[(characters_and_movies['cluster'] == n) & (characters_and_movies['revenue'] > 5e8)]\n",
    "    top = top.sort_values(by='revenue', ascending=False).head(5)\n",
    "    print(top[['title', 'character']])\n",
    "    plot_topic_distribution(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in top_clusters[:5]:\n",
    "    print_cluster_info(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that three most popular archetypes are all archetypes of different kinds of protagonists and their close allies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biggest range in normalized frequency\n",
    "\n",
    "top_diff_clusters = normalized_archetype_counts.apply(np.ptp).sort_values(ascending=False)[:10].index.values\n",
    "top_clusters_archetype_counts = normalized_archetype_counts[top_diff_clusters]\n",
    "top_clusters_archetype_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_clusters_archetype_counts.plot(figsize=(12, 6))\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title('Normalized character counts by cluster: subset 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------\n",
    "n = 10 # sliding average window size\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate over clusters and plot a line for each\n",
    "for cluster in top_diff_clusters:\n",
    "    x = top_clusters_archetype_counts[cluster]\n",
    "    x_avg = np.convolve(x, np.ones(n)/n, mode='valid')\n",
    "    y = top_clusters_archetype_counts.index\n",
    "    y_1 = y[round(n/2):-(n-round(n/2))+1]\n",
    "    plt.plot(y_1, x_avg, label=f'Cluster {cluster}', marker='', linewidth=0.7)\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(top_clusters_archetype_counts.index[::5], rotation=45, ha='right')\n",
    "plt.xlim([1931, 2013])\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Normalized character count')\n",
    "plt.title(f'Normalized character counts by cluster: subset 1. Sliding average (n={n})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in top_diff_clusters[:5]:\n",
    "    print_cluster_info(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The archetypes that changed in popularity the most are some side caracters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cultural preference\n",
    "\n",
    "We are interested in the cultural preferences at more modern times, so we will look only at the data from the 21 century. We also will use only the first country in the list of production countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_movies_countries = characters_and_movies[characters_and_movies['release_year'] > 2000]\n",
    "char_movies_countries['countries'] = char_movies_countries['countries'].apply(lambda x: x[0] if x else 'Unknown')\n",
    "\n",
    "char_movies_countries = char_movies_countries.groupby(['countries', 'cluster']).size().reset_index(name='character_count')\n",
    "\n",
    "char_movies_countries = char_movies_countries[['countries', 'cluster', 'character_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top production countries\n",
    "char_movies_countries.groupby('countries')['character_count'].sum().sort_values(ascending=False)[:50].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a lot of films with unknown country of production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_countries = char_movies_countries.groupby('countries')['character_count'].sum().sort_values(ascending=False)[:11].index\n",
    "top_countries = top_countries.drop('Unknown')\n",
    "top_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_movies_countries = char_movies_countries[char_movies_countries['countries'].isin(top_countries)]\n",
    "archetype_by_country = char_movies_countries.pivot(index='countries', columns='cluster', values='character_count').fillna(0)\n",
    "archetype_by_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_movies_countries.groupby('countries')['character_count'].sum().plot.barh(x='countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have much more data on the american films, so we will normalize the data. After that let's look at the distribution of the global top 5 archetypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_archetype_by_country = (archetype_by_country)/(archetype_by_country.values.sum(1).reshape(-1, 1))\n",
    "\n",
    "normalized_archetype_by_country[top_clusters[:5]].plot.barh(figsize=(7, 10), title='Distribution of the top 5 clusters in top 10 countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to notice the difference between this countries. We can notice the difference in the types of the most popular protagonists for example in India and Hong Kong compared to United Kingdom and Spain. In the first group, the most popular protagonists are those who act and achieve something, while in the second group the most popular protagonists are communicating more and travel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can't say, that the distribution of the archetypes is different in different countries. But was it the case in 20th century?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_movies_countries = characters_and_movies[characters_and_movies['release_year'] < 2001]\n",
    "char_movies_countries['countries'] = char_movies_countries['countries'].apply(lambda x: x[0] if x else 'Unknown')\n",
    "\n",
    "char_movies_countries = char_movies_countries.groupby(['countries', 'cluster']).size().reset_index(name='character_count')\n",
    "\n",
    "char_movies_countries = char_movies_countries[['countries', 'cluster', 'character_count']]\n",
    "\n",
    "char_movies_countries = char_movies_countries[char_movies_countries['countries'].isin(top_countries)]\n",
    "archetype_by_country = char_movies_countries.pivot(index='countries', columns='cluster', values='character_count').fillna(0)\n",
    "\n",
    "normalized_archetype_by_country = (archetype_by_country)/(archetype_by_country.values.sum(1).reshape(-1, 1))\n",
    "\n",
    "normalized_archetype_by_country[top_clusters[:5]].plot.barh(figsize=(7, 10), title='Distribution of the top 5 clusters in top 10 countries in the 20th century')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the shift that hapend from the 20th to 21st century from more achieving to communicating protagonists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie success based on the archetypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model for revenue prediction\n",
    "\n",
    "To determine importance of the archetypes for the movie success, we can build the linear model and tell what are the most important archetypes based on the coefficient and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_and_revenue = characters_and_movies[characters_and_movies['discounted_revenue'].notna()][['wiki_id', 'discounted_revenue']]\n",
    "cluster_and_revenue['log_revenue'] = np.log(cluster_and_revenue['discounted_revenue'])\n",
    "cluster_and_revenue = cluster_and_revenue[['wiki_id', 'log_revenue']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "revenues = cluster_and_revenue['log_revenue'].values\n",
    "wiki_ids = cluster_and_revenue['wiki_id'].values\n",
    "\n",
    "plt.hist(revenues, bins=50)\n",
    "plt.title('Log revenue histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Obtain the data\n",
    "categories_movie_plot = [\"Movies with Revenue\", \"Movies with Rating\", \"Movies with Rating and Revenue\"]\n",
    "\n",
    "movie_values = [movies['revenue'].notna().sum(),\n",
    "          len(movies_with_rating),\n",
    "          movies_with_rating['revenue'].notna().sum()]\n",
    "\n",
    "# Create a bar chart\n",
    "fig = go.Figure(data=[go.Bar(x=categories_movie_plot, y=movie_values)])\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "    title=\"Statistics of Movies with Revenue and Rating\",\n",
    "    xaxis_title=\"Categories\",\n",
    "    yaxis_title=\"Number of Movies\",\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-20T14:26:29.590194Z",
     "iopub.status.busy": "2023-12-20T14:26:29.589988Z",
     "iopub.status.idle": "2023-12-20T14:26:32.128043Z",
     "shell.execute_reply": "2023-12-20T14:26:32.127317Z"
    }
   },
   "outputs": [],
   "source": [
    "clusters = np.zeros((len(cluster_and_revenue), 50)) #initializing the clusters\n",
    "for i in range(len(wiki_ids)):\n",
    "    wiki_id = wiki_ids[i]\n",
    "    for c in characters_and_movies[characters_and_movies['wiki_id'] == wiki_id]['cluster'].values:\n",
    "        clusters[i][c] = 1\n",
    "        \n",
    "\n",
    "cluster_revenue_data = pd.DataFrame(clusters, columns=[f'archetype_{i}' for i in np.arange(50)])\n",
    "cluster_revenue_data['log_revenue'] = cluster_and_revenue['log_revenue']\n",
    "cluster_revenue_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "\n",
    "model_str = \"log_revenue ~ \"\n",
    "for i in range(n):\n",
    "    model_str += \"C(archetype_\" + str(i) + \")+\"\n",
    "\n",
    "model_str_without_interaction = model_str.strip(\"+\")\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        model_str += \"C(archetype_\" + str(i) + \"):C(archetype_\" + str(j) + \")+\"\n",
    "\n",
    "model_str = model_str.strip(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula = model_str_without_interaction, data = cluster_revenue_data)\n",
    "res_without_interaction = mod.fit()\n",
    "res_without_interaction.summary().tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smf.ols(formula = model_str, data = cluster_revenue_data)\n",
    "res = mod.fit()\n",
    "res.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the significant improvement in R-squared metric, we can say that interactions between archetypes are important.\n",
    "\n",
    "Next, let's look at anova results to determine the important archetypes and interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(res, robust='hc3').sort_values('PR(>F)')[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actors success based on the archetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rating_revenue = movies_with_rating[['wiki_id', 'discounted_revenue', 'averageRating']].dropna(subset=['discounted_revenue']).reset_index(drop=True)\n",
    "movies_rating_revenue['discounted_revenue'] = np.log(movies_rating_revenue['discounted_revenue'])\n",
    "movies_rating_revenue['norm_log_revenue'] = (movies_rating_revenue['discounted_revenue'] - np.min(movies_rating_revenue['discounted_revenue'])) * 10/ (np.max(movies_rating_revenue['discounted_revenue']) - np.min(movies_rating_revenue['discounted_revenue']))\n",
    "movies_rating_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rating_revenue[['norm_log_revenue', 'averageRating']].plot.hist(alpha=0.3, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearsonr(movies_rating_revenue['norm_log_revenue'].values, movies_rating_revenue['averageRating'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that there is some statistically significant correlation, even though it's not wery big. We will use the sum of normalized log revenue and rating of the film as the metric for success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rating_revenue['success'] = movies_rating_revenue['averageRating'] + movies_rating_revenue['norm_log_revenue']\n",
    "movies_rating_revenue['success'].plot.hist(alpha=0.3, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_and_characters['importance'] = actors_and_characters['adj'].apply(len) + actors_and_characters['active'].apply(len) + actors_and_characters['patient'].apply(len)\n",
    "actors_and_characters['importance'] = actors_and_characters['importance'] / actors_and_characters.groupby('wiki_id')['importance'].transform('sum')\n",
    "actors_and_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_and_characters_with_success = actors_and_characters.merge(movies_rating_revenue[['wiki_id', 'success']], left_on='wiki_id', right_on='wiki_id')\n",
    "actors_and_characters_with_success = actors_and_characters_with_success[['actor', 'cluster', 'date_of_birth', 'sex', 'height', 'age', 'importance', 'success']]\n",
    "actors_and_characters_with_success.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_and_characters_with_success['weighted_success'] = actors_and_characters_with_success['importance'] * actors_and_characters_with_success['success']\n",
    "top_actors = actors_and_characters_with_success.groupby('actor').agg({'weighted_success': ['sum', 'size'], 'cluster':list, 'sex':'last', 'date_of_birth':'last'}).sort_values(('weighted_success',  'sum'), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only look at the actors with at least 5 films\n",
    "top_actors.columns = ['sum_success', 'num_films', 'clusters', 'sex', 'date_of_birth']\n",
    "top_actors = top_actors.reset_index()\n",
    "top_actors = top_actors[top_actors['num_films'] > 4]\n",
    "top_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_actors['year_of_birth'] = top_actors['date_of_birth'].apply(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sum_success_plot = sns.scatterplot(data=top_actors[:50], x='actor', y='sum_success', hue='num_films')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Archetype number and actor's success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_actors['cluster_number'] = top_actors['clusters'].apply(lambda x: len(set(x)))\n",
    "top_actors['clusters_to_films_ratio'] = top_actors['cluster_number']/top_actors['num_films']\n",
    "top_actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(top_actors['clusters_to_films_ratio'], bins=20)\n",
    "print('Median of the cluster to film ratio: ', np.median(top_actors['clusters_to_films_ratio']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_actors['many_archetypes'] = top_actors['clusters_to_films_ratio'].apply(lambda x: int(x > 0.70))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform causal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_top_actors_data = top_actors[['sum_success', 'many_archetypes', 'year_of_birth', 'sex', 'num_films']].reset_index(drop=True)\n",
    "norm_top_actors_data['sex'] =  norm_top_actors_data['sex'].apply(lambda x: int(x=='F'))\n",
    "norm_top_actors_data['year_of_birth'] = norm_top_actors_data['year_of_birth'] - norm_top_actors_data['year_of_birth'].mean() / norm_top_actors_data['year_of_birth'].std()\n",
    "norm_top_actors_data['num_films'] = norm_top_actors_data['num_films'] - norm_top_actors_data['num_films'].mean() / norm_top_actors_data['num_films'].std()\n",
    "\n",
    "\n",
    "mod = smf.logit(formula='many_archetypes ~  year_of_birth + sex + num_films', data=norm_top_actors_data)\n",
    "res = mod.fit()\n",
    "\n",
    "# Extract the estimated propensity scores\n",
    "norm_top_actors_data['Propensity_score'] = res.predict()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    '''Calculate similarity for instances with given propensity scores'''\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)\n",
    "\n",
    "treatment_df = norm_top_actors_data[norm_top_actors_data['many_archetypes'] == 1]\n",
    "control_df = norm_top_actors_data[norm_top_actors_data['many_archetypes'] == 0]\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "    for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "\n",
    "        similarity = get_similarity(control_row['Propensity_score'],\n",
    "                                    treatment_row['Propensity_score'])\n",
    "\n",
    "        G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "\n",
    "matching = nx.max_weight_matching(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "balanced_norm_top_actors_data = norm_top_actors_data.iloc[matched]\n",
    "balanced_norm_top_actors_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treated = balanced_norm_top_actors_data.loc[balanced_norm_top_actors_data['many_archetypes'] == 1]\n",
    "control = balanced_norm_top_actors_data.loc[balanced_norm_top_actors_data['many_archetypes'] == 0]\n",
    "\n",
    "ax = sns.histplot(treated['sum_success'], kde=True, stat='density', color='blue', label='many_archetypes');\n",
    "ax = sns.histplot(control['sum_success'], kde=True, stat='density', color='orange', label='not many_archetypes')\n",
    "ax.set(title='Succes distribution comparison',xlabel='sum success', ylabel='density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(treated['sum_success'],control['sum_success'], alternative='less')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can say that ators, who have played less archetypes are statistically significantly more successful that those, who played more various archetypes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
